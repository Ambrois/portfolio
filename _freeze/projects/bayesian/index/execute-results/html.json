{
  "hash": "f9814a216ee9c01cd5e5b7434b308985",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Analysis of Bayesian Model Misspecification\"\nauthors: \"Matthew Huang, Siya Shenoi\"\noutput: html_document\ndate: \"2025-06-03\"\ndescription: \"Comparing Bayesian predictive distributions under examples of model misspecification.\"\ncategories: [\"statistics\"]\nexecute:\n  cache: true      # cache chunk results\n  freeze: auto     # skip re-execution unless the source changes\nformat:\n  html:\n    theme: cosmo\n---\n\n\n\n## Introduction\n\nBayesian methods combine prior beliefs with observed data to update uncertainty, but they are still vulnerable to model misspecification, where assumed structure (like independence or hierarchy) does not match the true data-generating process.\n\nThis project asks: if two models have nearly the same prior predictive behavior, how much can mild structural misspecification still change posterior predictions?\n\nWe test two cases:\n\n1. Data are hierarchical, but the analysis model is flat.\n2. Data are dependent, but the analysis model assumes independence.\n\nIn both cases, posterior predictive distributions diverge meaningfully after seeing the same data. The practical takeaway is that a model can look well calibrated before observing data and still give misleading posterior inferences if its structure is wrong.\n\n## Example 1. Real data is hierarchical, but assumed to be flat.\n\nOur data $y$ follows the following hierarchical distribution, which we'll denote $D_1$.\n\n$$\n\\begin{align}\nY &\\sim Binom(100, p) \\\\\np &\\sim Beta(3, 4)\n\\end{align}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_obs = 100\ny = r_beta_bernoulli(n_obs, alpha = 3, 4)\ny_obs = sum(y)\ny_obs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 42\n```\n\n\n:::\n:::\n\n\n\nNow, we will misspecify the model.\n\nWe now assume the data comes from a binomial distribution, and assign a beta(5,3) prior on the proportion, where Y is the sum of the data,\nWe'll call this model $M$ (for \"Misspecified\").\n$$\n\\begin{aligned}\nY &\\sim \\text{Binom}(n = 100, p) \\\\\n(Prior)\\quad p &\\sim \\text{Beta}(5, 3)\n\\end{aligned}\n$$\n\nPrior Predictive distribution:\n\n::: {.cell}\n\n```{.r .cell-code}\npp_sample_size = 10000\n\n# the predictive distribution follows a beta-binomial\nprior_pred_M = r_beta_binom(pp_sample_size, n_obs, 5, 3)\n```\n:::\n\n\n\nNow to find an equivalent prior for the real model\n\nThe real model ($D_1$):\n$$\n\\begin{aligned}\nY &\\sim \\text{Binom}(n=100, p) \\\\\np &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n(Prior)\\quad \\alpha &\\sim \\log \\mathcal{N}(\\mu_1, \\sigma_1) \\\\\n(Prior)\\quad \\beta &\\sim \\log \\mathcal{N}(\\mu_2, \\sigma_2)\n\\end{aligned}\n$$\n\nA function to sample from the prior predictive distribution:\n\n::: {.cell}\n\n```{.r .cell-code}\n# function for sampling from prior predictive distrib of D2 model\nsample_from_pp_D1 = function(n, size, mu1, sigma1, mu2, sigma2) {\n  alphas = rlnorm(n, mu1, sigma1)\n  betas = rlnorm(n, mu2, sigma2)\n  probs = rbeta(n, alphas, betas)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n```\n:::\n\n\n\nWe will use lognormal $\\log \\mathcal{N}(\\log 5,\\ 0.01)$ and $\\log \\mathcal{N}(\\log 3,\\ 0.01)$ as the priors for $\\alpha$ and $\\beta$ in model $D_1$, chosen to create a very similar prior predictive distribution to the prior for $M$.\n\nThese are the prior predictive distributions compared:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_pred_D1 = sample_from_pp_D1(n = pp_sample_size,\n                                 size = n_obs,\n                                 mu1 = log(5), sigma1 = 0.01,\n                                 mu2 = log(3), sigma2 = 0.01)\n\ntvd_prior = pp_sample_distance(prior_pred_D1, prior_pred_M)\ntvd_prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0447\n```\n\n\n:::\n\n```{.r .cell-code}\nplot_pp_difference(real_sample = prior_pred_D1, real_model_name = \"D1\",\n                   miss_sample = prior_pred_M, miss_model_name = \"M\",\n                   prior_or_post = \"Prior\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nAs shown, the prior predictive distributions are extremely similar, so these priors are effectively equivalent.\n\nThat means any posterior differences after updating on data should mostly come from model structure, not prior choice.\n\nNow we update both models and compare the posterior predictives.\n\n### Updating $D_1$\n\nWe approximate the posterior with naive simulation (rejection sampling).\n\n::: {.cell}\n\n```{.r .cell-code}\n# We sample from the prior a lot, predict with the param values from the prior,\n# and keep if the prediction is equal to the observed data\n\ndf = data.frame(alpha = numeric(), beta = numeric())\n\ni = 1\nwhile (i <= pp_sample_size) {\n  alpha = rlnorm(1, log(5), 0.01)\n  beta = rlnorm(1, log(3), 0.01)\n  \n  y_pred = r_beta_binom(1, n_obs, alpha, beta)\n  \n  if (y_pred == y_obs) {\n    df = rbind(df, data.frame(alpha=alpha, beta=beta))\n    i = i+1\n  }\n}\n\ndf <- mutate(df, post_pred = r_beta_binom(n(), n_obs, alpha, beta))\n```\n:::\n\n\n\n### Updating $M$\nThis is a straightforward application of the Beta-Binomial Prior.\n\n$$\nY \\sim Binom(100, p), \\quad p \\sim Beta(5, 3)\n$$\n\nso\n$$\np \\mid Y = 42 \\sim Beta\\!\\big(5+42,\\ 3+58\\big)\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n```\n:::\n\n\n\n### Comparing analysis for $M$ vs equivalent analysis for $D_1$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M)\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D1\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThe posterior predictives are clearly different, with total variation distance 0.648 (a 14.4966443-fold increase over prior TVD). In this setup, misspecification has a meaningful effect on inference.\n\n## Example 2. Real data is dependent, but assumed to be independent.\n\nNow we test a dependence misspecification case. Data are generated by summing a Markov process of $n$ drifting Bernoulli trials (model $D_2$).\n\n$$\nY \\sim MC(n=100, \\ p_0=0.4, \\ \\delta=0.1)\n$$\n(MC = Markov Counts; sampling details are in helper functions.)\n\nHere, $p_0$ is the initial success probability and $\\delta$ is the standard deviation of Gaussian drift added to the success probability after each trial.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_ops = 100\ny = r_markov_bernoulli(n_ops, p0=0.4, drift_sd=0.1)\ny_obs = sum(y)\ny_obs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 34\n```\n\n\n:::\n:::\n\n\n\nAgain, we intentionally misspecify by fitting a binomial model with the same $\\text{Beta}(5, 3)$ prior on $p$ (model $M$).\n$$\n\\begin{aligned}\nY &\\sim \\text{Binom}(n=100, p) \\\\\n(Prior)\\quad p &\\sim \\text{Beta}(5, 3)\n\\end{aligned}\n$$\n\nNow we choose an equivalent prior for model $D_2$.\n$$\n\\begin{aligned}\nY &\\sim MC(n=100, p_0, \\delta) \\\\\n(Prior)\\quad p_0 &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n(Prior)\\quad \\delta &\\sim \\text{Exp}(\\lambda)\n\\end{aligned}\n$$\nWe use the function below to sample from the prior predictive distribution of $D_2$.\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_from_pp_D2 = function(n, size, alpha, beta, rate) {\n    p0 = rbeta(n, alpha, beta)\n    drift_sd = rexp(n, rate)\n    sample = r_markov_counts(n, size, p0, drift_sd)\n    return(sample)\n}\n```\n:::\n\n\nWe use this prior distribution:\n$$\n\\begin{aligned}\nY &\\sim MC(n=100, \\ p_0, \\ \\delta) \\\\\n(Prior)\\quad p_0 &\\sim \\text{Beta}(5, 3) \\\\\n(Prior)\\quad \\delta &\\sim \\text{Exp}(100)\n\\end{aligned}\n$$\n\nNow compare the prior predictive distributions:\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_pred_D2 = sample_from_pp_D2(n=pp_sample_size,\n                                  size = n_obs,\n                                  alpha=5, beta=3, rate=100)\n\ntvd_prior = pp_sample_distance(prior_pred_D2, prior_pred_M)\ntvd_prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0601\n```\n\n\n:::\n\n```{.r .cell-code}\nplot_pp_difference(real_sample = prior_pred_D2, real_model_name=\"D2\",\n                   miss_sample = prior_pred_M, miss_model_name=\"M\",\n                   prior_or_post = \"Prior\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nAgain, the prior predictives are effectively equivalent.\n\nNext we update both models and measure how much the posterior predictives diverge.\n\n### Updating $D_2$\n\nWe use naive simulation again to get a posterior sample for $D_2$.  \nThis step is slower, so we accept draws within 1 of the observed count, which makes the approximation slightly rougher.\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = data.frame(p0 = numeric(), drift_sd = numeric())\n\ni = 1\nwhile (i <= pp_sample_size) {\n  p0 = rbeta(1, 5, 3)\n  drift_sd = rexp(1, 100)\n  \n  y_pred = r_markov_counts(1, n_obs, p0, drift_sd)\n  \n  if (abs(y_pred - y_obs) <= 1) {\n    df = rbind(df, data.frame(p0=p0, drift_sd=drift_sd))\n    i = i+1\n  }\n}\n\ndf <- mutate(df, post_pred = r_markov_counts(n(), n_obs, p0, drift_sd))\n```\n:::\n\n\n### Updating $M$\nAgain, this is a straightforward Beta-Binomial update.\n\n::: {.cell}\n\n```{.r .cell-code}\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n```\n:::\n\n\n### Comparing Posterior Predictive Distributions:\n\n::: {.cell}\n\n```{.r .cell-code}\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M, method = 'tvd')\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D2\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nThe total variation distance is 0.214 (a 3.5607321-fold increase from prior TVD), and the posterior predictive distributions are visibly different.\n\n\n## Conclusion\n\nThese examples show that mild misspecification can materially change Bayesian conclusions, even when prior predictive checks look reasonable. In practice, this is a warning: prior calibration is helpful, but it does not protect you if structural assumptions are wrong.\n\n### Future Work\n\nA next step is to identify which analyses and test statistics stay robust under different kinds and degrees of misspecification. To do that, we need:\n\n- A principled way to quantify how far a proposed model is from the true data-generating process.\n- An efficient method for choosing priors on the true model that match the proposed modelâ€™s prior predictive distribution, so structural effects can be isolated cleanly. (2)\n\n(2) In practice, matching a target prior predictive distribution is hard because the prior search space is large. A more general and computationally efficient search method would make this workflow much more usable.\n\n\n## Appendix\n\n### Helper Functions\n\nFunctions are sourced at the top so they stay out of the way while reading. Full helper code is shown here for reference.\n\n```r\n r_beta_bernoulli = function(n, alpha, beta) {\n  probs = rbeta(n, alpha, beta)\n  sample = rbinom(n, 1, probs)\n  return(sample)\n}\n\nr_beta_binom = function(n, size, alpha, beta) {\n  probs = rbeta(n, alpha, beta)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nr_markov_bernoulli = function(n, p0, drift_sd) {\n  probs = numeric(n)\n  probs[1] = p0\n  \n  for (i in 2:n) {\n    probs[i] = probs[i - 1] + rnorm(1, 0, drift_sd)\n    probs[i] = min(max(probs[i], 0), 1)\n  }\n  sample = rbinom(n, 1, probs)\n  return(sample)\n}\n\nr_markov_counts = function(n, size, p0, drift_sd) {\n  sample = numeric(n)\n  for (i in 1:n) {\n    sample[i] = sum(r_markov_bernoulli(size, p0[i], drift_sd[i]))\n  }\n  return(sample)\n}\n\npp_sample_distance = function(pp_sample1, pp_sample2, method = \"tvd\") {\n  \n  if (length(pp_sample1) != length(pp_sample2)) {\n    print(\"warning: sample sizes are different\")\n  }\n  \n  if (method == \"tvd\") {\n    max_val = max(c(pp_sample1, pp_sample2))\n    \n    counts1 = tabulate(pp_sample1, nbins = max_val)\n    counts2 = tabulate(pp_sample2, nbins = max_val)\n    \n    counts1 = counts1 / sum(counts1)\n    counts2 = counts2 / sum(counts2)\n    \n    return(0.5 * sum(abs(counts1 - counts2)))\n  } else if (method == \"binned_tvd\") {\n    combined = c(pp_sample1, pp_sample2)\n    breaks = hist(combined, plot = FALSE)$breaks\n  \n    counts1 = hist(pp_sample1, breaks = breaks, plot = FALSE)$counts\n    counts2 = hist(pp_sample2, breaks = breaks, plot = FALSE)$counts\n    \n    counts1 = counts1 / sum(counts1)\n    counts2 = counts2 / sum(counts2)\n    \n    return(0.5 * sum(abs(counts1 - counts2)))\n  } else {\n    print(\"available methods are 'tvd' and 'binned_tcd'\")\n  }\n}\n\nplot_pp_difference = function(real_sample, miss_sample, prior_or_post, real_model_name, miss_model_name) {\n  \n  plot_df = tibble(\n   D = real_sample,\n   M = miss_sample\n  ) |> \n  pivot_longer(\n    cols = D:M,\n    names_to = \"source\",\n    values_to = \"values\"\n  )\n\n  ggplot(plot_df, aes(x = values, fill = source)) +\n    geom_histogram(aes(y = after_stat(density)), position = \"identity\", alpha = 0.5, bins = 30) +\n    labs(title = glue(\"{prior_or_post} Predictive Distribution Comparison\"), x = \"Count\", y = \"Density\") +\n    scale_fill_discrete(\n      labels = c(glue(\"Real Model ({real_model_name})\"), glue(\"Misspecified Model ({miss_model_name})\"))) + \n    theme_minimal()\n}\n\nsample_from_pp_D1 = function(n, size, mu1, sigma1, mu2, sigma2) {\n  alphas = rlnorm(n, mu1, sigma1)\n  betas = rlnorm(n, mu2, sigma2)\n  probs = rbeta(n, alphas, betas)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nsample_from_pp_D2 = function(n, size, alpha, beta, rate) {\n    p0 = rbeta(n, alpha, beta)\n    drift_sd = rexp(n, rate)\n    sample = r_markov_counts(n, size, p0, drift_sd)\n    return(sample)\n}\n\nr_markov_bernoulli_old = function(n, p_initial, p_after_1, p_after_0){\n  sample = numeric(n)\n  sample[1] = rbinom(1, 1, p_initial)\n  for (i in 2:n) {\n    if (sample[i - 1] == 1) {sample[i] = rbinom(1, 1, p_after_1)}\n    else {sample[i] = rbinom(1, 1, p_after_0)}\n  }\n  return(sample)\n}\n\nr_markov_counts_old = function(n, size, p_initial, p_after_1, p_after_0) {\n  sample = numeric(n)\n  for (i in 1:n) {\n    sample[i] = sum(r_markov_bernoulli_old(size, p_initial, p_after_1, p_after_0))\n  }\n  return(sample)\n} \n```\n\n### Errata\n\nWe first tried grid search over parameterized priors to match prior predictives, but it performed poorly.\n\n::: {.cell}\n\n```{.r .cell-code}\n# grid = expand.grid(\n#   mu1 = seq(1, 50, by=5),\n#   mu2 = seq(1, 50, by=5),\n#   sigma1 = seq(1, 50, by=5),\n#   sigma2 = seq(1, 50, by=5)\n# )\n\n# grid = grid |> \n#   mutate(\n#     pp_sample = pmap(\n#       list(mu1, sigma1, mu2, sigma2),\n#       ~ sample_from_pp_D1(n = pp_sample_size,\n#                           size = n_obs,\n#                           mu1 = ..1, sigma1 = ..2,\n#                           mu2 = ..3, sigma2 = ..4))) |> \n#   mutate(\n#     distances = map(pp_sample, ~ pp_sample_distance(.x, prior_pred_M))\n#   ) |> \n#   arrange(distances)\n```\n:::\n\n\n\nWe also tried a different Markov model variant, but could not find priors that produced equivalent prior predictives.\n\n::: {.cell}\n\n```{.r .cell-code}\n# (UNUSED because couldn't find equivalent predictive prior) function to sample Markov Bernoulli Distribution\nr_markov_bernoulli_old = function(n, p_initial, p_after_1, p_after_0){\n  sample = numeric(n)\n  sample[1] = rbinom(1,1,p_initial)\n  for (i in 2:n) {\n    if (sample[i-1]==1) {sample[i] = rbinom(1,1, p_after_1)}\n    else {sample[i] = rbinom(1,1, p_after_0)}\n  }\n  return(sample)\n}\n\n# (UNUSED because couldn't find equivalent predictive prior) function to sample counts from 'size'-number of markov bernoulli trials\nr_markov_counts_old = function(n, size, p_initial, p_after_1, p_after_0) {\n  sample = numeric(n)\n  for (i in 1:n){ # for each sample, do 'size' bernoulli trials, then sum\n    sample[i] = sum(r_markov_bernoulli(size, p_initial, p_after_1, p_after_0))\n  }\n  return(sample)\n}\n```\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}