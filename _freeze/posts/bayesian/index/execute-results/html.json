{
  "hash": "fa60806430926950472a6d506b63d9b3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Analysis of Examples of Bayesian Model Misspecification\"\nauthors: \"Matthew Huang, Siya Shenoi\"\noutput: html_document\ndate: \"2025-06-03\"\ndescription: \"Comparing bayesian predictive distributions under examples of model misspecification.\"\nexecute:\n  cache: true      # cache chunk results\n  freeze: auto     # skip re-execution unless the source changes\n---\n\n\n\n### Introduction\n\nThe accuracy of Bayesian inference depends on how well the chosen model captures the underlying data-generating process. However, in practice, models are often misspecified, with assumptions violated in various degrees. This analysis looks at two examples of model misspecification, comparing predictive distributions from correctly specified models with those from simpler misspecified alternatives. \n\nTo facilitate accurate comparison, we match prior predictive distributions across models, which means that observed discrepancies in the posterior arise primarily due to model misspecification. To compare predictive distributions, we use total variation distance (TVD) and visualizalizations. We use naive rejection sampling for posterior sampling because we lack an explicit formula for likelihood for the dependent model, parameter dimensions are small, and for consistency across examples.\n\nTwo examples are considered:\n\n1. When the true data-generating process is hierarchical (Beta-Binomial).\n\n2. When the true process involves dependent observations modeled as a drifting Markov chain.\n\n## Example 1. Real Data is Hierarchical\n\nOur data y follows the following hierarchical distribution, which we'll denote D1.\n\n$$\nY \\sim Binom(100, p) \\\\\np \\sim Beta(3, 4)\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_obs = 100\ny = r_beta_bernoulli(n_obs, alpha = 3, 4)\ny_obs = sum(y)\ny_obs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 42\n```\n\n\n:::\n:::\n\n\n\nNow, we will misspecify the model.\n\nWe now assume the data comes from a binomial distribution, and assign a beta(5,3) prior on the proportion, where Y is the sum of the data,\nWe'll call this model M (for \"Misspecified\").\n$$\nY \\sim Binom(n=100, p) \\\\\n(Prior) \\quad p \\sim Beta(5, 3)\n$$\n\nPrior Predictive distribution:\n\n::: {.cell}\n\n```{.r .cell-code}\npp_sample_size = 10000\n\n# the predictive distribution follows a beta-binomial\nprior_pred_M = r_beta_binom(pp_sample_size, n_obs, 5, 3)\n```\n:::\n\n\n\nNow to find an equivalent prior for the real model\n\nThe real model (D1):\n$$\nY \\sim Binom(n=100, p) \\\\\np \\sim Beta(\\alpha, \\beta) \\\\\n(Prior) \\quad \\alpha \\sim logN(\\mu_1, \\sigma_1) \\\\\n(Prior) \\quad \\beta \\sim logN(\\mu_2, \\sigma_2)\n$$\n\nA function to sample from the prior predictive distribution:\n\n::: {.cell}\n\n```{.r .cell-code}\n# function for sampling from prior predictive distrib of D2 model\nsample_from_pp_D1 = function(n, size, mu1, sigma1, mu2, sigma2) {\n  alphas = rlnorm(n, mu1, sigma1)\n  betas = rlnorm(n, mu2, sigma2)\n  probs = rbeta(n, alphas, betas)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n```\n:::\n\n\n\nWe will use logN(log(5),0.01) and logN(log(3),0.01) as the priors for alpha and beta in model D1, it was chosen specifically to create a very similar prior predictive distribution to the prior for M.\n\nThese are the prior predictive distributions compared:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_pred_D1 = sample_from_pp_D1(n = pp_sample_size,\n                                 size = n_obs,\n                                 mu1 = log(5), sigma1 = 0.01,\n                                 mu2 = log(3), sigma2 = 0.01)\n\ntvd_prior = pp_sample_distance(prior_pred_D1, prior_pred_M)\ntvd_prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0447\n```\n\n\n:::\n\n```{.r .cell-code}\nplot_pp_difference(real_sample = prior_pred_D1, real_model_name = \"D1\",\n                   miss_sample = prior_pred_M, miss_model_name = \"M\",\n                   prior_or_post = \"Prior\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nAs you can see, the predictive distributions for the two models are extremely similar, which suggests the two priors are effectively equivalent. \n\nThis implies that any differences observed in the posterior distributions after updating on data are likely attributable to differences in the models themselves, rather than the priors.\n\nNow let's find the posteriors of each and see how they vary.\n\n### Updating D1\n\nFinding posterior via naive simulation\n\n::: {.cell}\n\n```{.r .cell-code}\n# We sample from the prior a lot, predict with the param values from the prior, and keep if the prediction is equal to the observed data\n\ndf = data.frame(alpha = numeric(), beta = numeric())\n\ni = 1\nwhile (i <= pp_sample_size) {\n  alpha = rlnorm(1, log(5), 0.01)\n  beta = rlnorm(1, log(3), 0.01)\n  \n  y_pred = r_beta_binom(1, n_obs, alpha, beta)\n  \n  if (y_pred == y_obs) {\n    df = rbind(df, data.frame(alpha=alpha, beta=beta))\n    i = i+1\n  }\n}\n\ndf <- mutate(df, post_pred = r_beta_binom(n(), n_obs, alpha, beta))\n```\n:::\n\n\n\n### Updating M\nThis is a very straightforward application of the Beta-Binomial Prior.\n\nY ~ binomial(100, p), with p ~ Beta(5,3)\n\nso p|Y=42` ~ Beta(5+42, 3+58)\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n```\n:::\n\n\n\n### Comparing analysis for M vs equivalent analysis for D1\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M)\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D1\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThe posterior predictive distributions are quite different, with a Total Variational Distance of 0.648 (a 14.4966443 time increase from the prior TVD), so the model misspecification in this case seems to have a significant impact on results.\n\n## Example 2. Real Data is Dependent\n\nNow, we explore what happens if our sample comes from a dependent model. Specifically, we generate a sample from summing a Markov Chain of $n$ drifting Bernoulli trials, denoted as model D2.\n\n$$\nY \\sim MC(n=100, \\ p_0=0.4, \\ \\delta=0.1)\n$$\n(MC for Markov Counts, details for sampling are in the helper functions section)\n\nWhere for $n$ trials, $p_0$ is the initial probability of success, and $\\delta$ is the standard deviation of gaussian drift added to the success probability after each trial.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_ops = 100\ny = r_markov_bernoulli(n_ops, p0=0.4, drift_sd=0.1)\ny_obs = sum(y)\ny_obs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 34\n```\n\n\n:::\n:::\n\n\n\nAgain, we will misspecify the model by assuming the data comes from a binomial distribution. We assign the same beta(5,3) prior on $p$. Again, we call this model M.\n$$\nY \\sim Binom(n=100, p) \\\\\n(Prior) \\quad p \\sim Beta(5, 3)\n$$\n\nLet's then find an equivalent prior for model D2.\n$$\nY \\sim MC(n=100, p_0, \\delta) \\\\\n(Prior) \\quad p_0 \\sim Beta(\\alpha, \\beta) \\\\\n(Prior) \\quad \\delta \\sim  Exp(\\lambda)\n$$\nWe'll need a function to sample a prior predictive distribution for D2.\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_from_pp_D2 = function(n, size, alpha, beta, rate) {\n    p0 = rbeta(n, alpha, beta)\n    drift_sd = rexp(n, rate)\n    sample = r_markov_counts(n, size, p0, drift_sd)\n    return(sample)\n}\n```\n:::\n\n\nLet's use this prior distribution:\n$$\nY \\sim MC(n=100, \\ p_0, \\ \\delta) \\\\\n(Prior) \\quad p_0 \\sim Beta(5, 3) \\\\\n(Prior) \\quad \\delta \\sim Exp(100)\n$$\n\nAnd compare the prior predictive distributions:\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_pred_D2 = sample_from_pp_D2(n=pp_sample_size,\n                                  size = n_obs,\n                                  alpha=5, beta=3, rate=100)\n\ntvd_prior = pp_sample_distance(prior_pred_D2, prior_pred_M)\ntvd_prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0601\n```\n\n\n:::\n\n```{.r .cell-code}\nplot_pp_difference(real_sample = prior_pred_D2, real_model_name=\"D2\",\n                   miss_sample = prior_pred_M, miss_model_name=\"M\",\n                   prior_or_post = \"Prior\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nAs shown, the priors are effectively equivalent.\n\nLet's proceed with updating both models to see what kind of discrepancy there is.\n\n### Updating D2\n\nWe use naive simulation again to create a posterior sample for D2.\n(This one is much slower, so we're also accepting if the predicted is within 1 of the observed, which makes the posterior a bit less accurate.)\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = data.frame(p0 = numeric(), drift_sd = numeric())\n\ni = 1\nwhile (i <= pp_sample_size) {\n  p0 = rbeta(1, 5, 3)\n  drift_sd = rexp(1, 100)\n  \n  y_pred = r_markov_counts(1, n_obs, p0, drift_sd)\n  \n  if (abs(y_pred - y_obs) <= 1) {\n    df = rbind(df, data.frame(p0=p0, drift_sd=drift_sd))\n    i = i+1\n  }\n}\n\ndf <- mutate(df, post_pred = r_markov_counts(n(), n_obs, p0, drift_sd))\n```\n:::\n\n\n### Updating M\nAgain, a straightforward updating of the Beta-Binomial Prior.\n\n::: {.cell}\n\n```{.r .cell-code}\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n```\n:::\n\n\n### Comparing Posterior Predictive Distributions:\n\n::: {.cell}\n\n```{.r .cell-code}\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M, method = 'tvd')\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D2\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nThere is a Total Variational Distance of 0.214 (a 3.5607321 time increase from the prior TVD), and we can see from the plot, the posterior predictive distributions are visibly different.\n\n\n## Conclusion\n\nThese examples demonstrate that model misspecification can have significant impacts of the results of a bayesian analyses. In terms of real world implications, it's unclear how realistic the examples we've used are, but it can still serve as a cautionary tale to not blindly trust your model.\n\nIt has proven to be very difficult to find prior distributions such that they have some specified prior predictive distribution. For future work, we could look for a more viable and general method of searching for priors.\n\n\n## Helper Functions (appendix)\n\nFunctions are sourced at the top so they stay out of the way while reading. Full helper code is shown here for reference.\n\n```r\n r_beta_bernoulli = function(n, alpha, beta) {\n  probs = rbeta(n, alpha, beta)\n  sample = rbinom(n, 1, probs)\n  return(sample)\n}\n\nr_beta_binom = function(n, size, alpha, beta) {\n  probs = rbeta(n, alpha, beta)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nr_markov_bernoulli = function(n, p0, drift_sd) {\n  probs = numeric(n)\n  probs[1] = p0\n  \n  for (i in 2:n) {\n    probs[i] = probs[i - 1] + rnorm(1, 0, drift_sd)\n    probs[i] = min(max(probs[i], 0), 1)\n  }\n  sample = rbinom(n, 1, probs)\n  return(sample)\n}\n\nr_markov_counts = function(n, size, p0, drift_sd) {\n  sample = numeric(n)\n  for (i in 1:n) {\n    sample[i] = sum(r_markov_bernoulli(size, p0[i], drift_sd[i]))\n  }\n  return(sample)\n}\n\npp_sample_distance = function(pp_sample1, pp_sample2, method = \"tvd\") {\n  \n  if (length(pp_sample1) != length(pp_sample2)) {\n    print(\"warning: sample sizes are different\")\n  }\n  \n  if (method == \"tvd\") {\n    max_val = max(c(pp_sample1, pp_sample2))\n    \n    counts1 = tabulate(pp_sample1, nbins = max_val)\n    counts2 = tabulate(pp_sample2, nbins = max_val)\n    \n    counts1 = counts1 / sum(counts1)\n    counts2 = counts2 / sum(counts2)\n    \n    return(0.5 * sum(abs(counts1 - counts2)))\n  } else if (method == \"binned_tvd\") {\n    combined = c(pp_sample1, pp_sample2)\n    breaks = hist(combined, plot = FALSE)$breaks\n  \n    counts1 = hist(pp_sample1, breaks = breaks, plot = FALSE)$counts\n    counts2 = hist(pp_sample2, breaks = breaks, plot = FALSE)$counts\n    \n    counts1 = counts1 / sum(counts1)\n    counts2 = counts2 / sum(counts2)\n    \n    return(0.5 * sum(abs(counts1 - counts2)))\n  } else {\n    print(\"available methods are 'tvd' and 'binned_tcd'\")\n  }\n}\n\nplot_pp_difference = function(real_sample, miss_sample, prior_or_post, real_model_name, miss_model_name) {\n  \n  plot_df = tibble(\n   D = real_sample,\n   M = miss_sample\n  ) |> \n  pivot_longer(\n    cols = D:M,\n    names_to = \"source\",\n    values_to = \"values\"\n  )\n\n  ggplot(plot_df, aes(x = values, fill = source)) +\n    geom_histogram(aes(y = after_stat(density)), position = \"identity\", alpha = 0.5, bins = 30) +\n    labs(title = glue(\"{prior_or_post} Predictive Distribution Comparison\"), x = \"Count\", y = \"Density\") +\n    scale_fill_discrete(\n      labels = c(glue(\"Real Model ({real_model_name})\"), glue(\"Misspecified Model ({miss_model_name})\"))) + \n    theme_minimal()\n}\n\nsample_from_pp_D1 = function(n, size, mu1, sigma1, mu2, sigma2) {\n  alphas = rlnorm(n, mu1, sigma1)\n  betas = rlnorm(n, mu2, sigma2)\n  probs = rbeta(n, alphas, betas)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nsample_from_pp_D2 = function(n, size, alpha, beta, rate) {\n    p0 = rbeta(n, alpha, beta)\n    drift_sd = rexp(n, rate)\n    sample = r_markov_counts(n, size, p0, drift_sd)\n    return(sample)\n}\n\nr_markov_bernoulli_old = function(n, p_initial, p_after_1, p_after_0){\n  sample = numeric(n)\n  sample[1] = rbinom(1, 1, p_initial)\n  for (i in 2:n) {\n    if (sample[i - 1] == 1) {sample[i] = rbinom(1, 1, p_after_1)}\n    else {sample[i] = rbinom(1, 1, p_after_0)}\n  }\n  return(sample)\n}\n\nr_markov_counts_old = function(n, size, p_initial, p_after_1, p_after_0) {\n  sample = numeric(n)\n  for (i in 1:n) {\n    sample[i] = sum(r_markov_bernoulli_old(size, p_initial, p_after_1, p_after_0))\n  }\n  return(sample)\n} \n```\n\n## Errata\n\nWe tried using grid search to find priors with equivalent prior predictive distributions, but it did not work well at all.\n\n::: {.cell}\n\n```{.r .cell-code}\n# grid = expand.grid(\n#   mu1 = seq(1, 50, by=5),\n#   mu2 = seq(1, 50, by=5),\n#   sigma1 = seq(1, 50, by=5),\n#   sigma2 = seq(1, 50, by=5)\n# )\n\n# grid = grid |> \n#   mutate(\n#     pp_sample = pmap(\n#       list(mu1, sigma1, mu2, sigma2),\n#       ~ sample_from_pp_D1(n = pp_sample_size,\n#                           size = n_obs,\n#                           mu1 = ..1, sigma1 = ..2,\n#                           mu2 = ..3, sigma2 = ..4))) |> \n#   mutate(\n#     distances = map(pp_sample, ~ pp_sample_distance(.x, prior_pred_M))\n#   ) |> \n#   arrange(distances)\n```\n:::\n\n\n\nWe tried to use a different kind of markov model originally, but could not find priors which made equivalent prior predictive distributions for it.\n\n::: {.cell}\n\n```{.r .cell-code}\n# (UNUSED because couldn't find equivalent predictive prior) function to sample Markov Bernoulli Distribution\nr_markov_bernoulli_old = function(n, p_initial, p_after_1, p_after_0){\n  sample = numeric(n)\n  sample[1] = rbinom(1,1,p_initial)\n  for (i in 2:n) {\n    if (sample[i-1]==1) {sample[i] = rbinom(1,1, p_after_1)}\n    else {sample[i] = rbinom(1,1, p_after_0)}\n  }\n  return(sample)\n}\n\n# (UNUSED because couldn't find equivalent predictive prior) function to sample counts from 'size'-number of markov bernoulli trials\nr_markov_counts_old = function(n, size, p_initial, p_after_1, p_after_0) {\n  sample = numeric(n)\n  for (i in 1:n){ # for each sample, do 'size' bernoulli trials, then sum\n    sample[i] = sum(r_markov_bernoulli(size, p_initial, p_after_1, p_after_0))\n  }\n  return(sample)\n}\n```\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}