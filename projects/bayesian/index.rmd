---
title: "Analysis of Bayesian Model Misspecification"
authors: "Matthew Huang, Siya Shenoi"
output: html_document
date: "2025-06-03"
description: "Comparing bayesian predictive distributions under examples of model misspecification."
categories: ["statistics"]
execute:
  cache: true      # cache chunk results
  freeze: auto     # skip re-execution unless the source changes
---

```{r setup, include=FALSE}
set.seed(0)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyverse)
library(glue)
source("helpers.R")
```

## Introduction

In real-world data analysis, models are almost never exactly correct. Simplifying assumptions, such as independence, homogeneity, or a specific distributional form, are routinely made to keep models tractable. While Bayesian methods are often viewed as robust because they incorporate uncertainty explicitly, they are still sensitive to model misspecification: where assumed model structure does not match the true data-generating process.

This project investigates how small, realistic forms of model misspecification can materially change Bayesian conclusions, even when prior assumptions appear reasonable. Rather than focusing on parameter estimates alone, we compare posterior predictive distributions, which directly reflect what the model believes future or unseen data will look like. This makes the analysis especially relevant for applied settings where Bayesian models are used to support forecasting, decision-making, or risk assessment.

To isolate the effect of misspecification, we construct pairs of models where:

- One model matches the true data-generating process.
- The other is a simpler, commonly used alternative.
- Both models are calibrated to have nearly identical prior predictive behavior.

By matching prior predictive distributions, any divergence observed after seeing data can be attributed primarily to differences in model structure rather than differences in prior assumptions. We quantify these differences using total variation distance and visualize how predictive beliefs diverge after updating on the same observations.

Two scenarios are examined:

1. Data generated from a hierarchical process but analyzed using a non-hierarchical model.
2. Data with dependence between observations but analyzed under an independence assumption.

Across both cases, we find that seemingly mild misspecification can lead to substantially different posterior predictions. The results highlight an important lesson for applied Bayesian modeling: a model that appears well-calibrated before seeing data may still produce misleading inferences if its structural assumptions are wrong.

## Example 1. Real data is hierarchical, but assumed to be flat.

Our data $y$ follows the following hierarchical distribution, which we'll denote $D_1$.

$$
\begin{align}
Y &\sim Binom(100, p) \\
p &\sim Beta(3, 4)
\end{align}
$$

```{r}
n_obs = 100
y = r_beta_bernoulli(n_obs, alpha = 3, 4)
y_obs = sum(y)
y_obs
```


Now, we will misspecify the model.

We now assume the data comes from a binomial distribution, and assign a beta(5,3) prior on the proportion, where Y is the sum of the data,
We'll call this model $M$ (for "Misspecified").
$$
\begin{aligned}
Y &\sim \text{Binom}(n = 100, p) \\
(Prior)\quad p &\sim \text{Beta}(5, 3)
\end{aligned}
$$

Prior Predictive distribution:
```{r}
pp_sample_size = 10000

# the predictive distribution follows a beta-binomial
prior_pred_M = r_beta_binom(pp_sample_size, n_obs, 5, 3)
```


Now to find an equivalent prior for the real model

The real model ($D_1$):
$$
\begin{aligned}
Y &\sim \text{Binom}(n=100, p) \\
p &\sim \text{Beta}(\alpha, \beta) \\
(Prior)\quad \alpha &\sim \log \mathcal{N}(\mu_1, \sigma_1) \\
(Prior)\quad \beta &\sim \log \mathcal{N}(\mu_2, \sigma_2)
\end{aligned}
$$

A function to sample from the prior predictive distribution:
```{r}
# function for sampling from prior predictive distrib of D2 model
sample_from_pp_D1 = function(n, size, mu1, sigma1, mu2, sigma2) {
  alphas = rlnorm(n, mu1, sigma1)
  betas = rlnorm(n, mu2, sigma2)
  probs = rbeta(n, alphas, betas)
  sample = rbinom(n, size, probs)
  return(sample)
}
```


We will use lognormal $\log \mathcal{N}(\log 5,\ 0.01)$ and $\log \mathcal{N}(\log 3,\ 0.01)$ as the priors for $\alpha$ and $\beta$ in model $D_1$, chosen to create a very similar prior predictive distribution to the prior for $M$.

These are the prior predictive distributions compared:

```{r}
prior_pred_D1 = sample_from_pp_D1(n = pp_sample_size,
                                 size = n_obs,
                                 mu1 = log(5), sigma1 = 0.01,
                                 mu2 = log(3), sigma2 = 0.01)

tvd_prior = pp_sample_distance(prior_pred_D1, prior_pred_M)
tvd_prior

plot_pp_difference(real_sample = prior_pred_D1, real_model_name = "D1",
                   miss_sample = prior_pred_M, miss_model_name = "M",
                   prior_or_post = "Prior")
```
As you can see, the predictive distributions for the two models are extremely similar, which suggests the two priors are effectively equivalent. 

This implies that any differences observed in the posterior distributions after updating on data are likely attributable to differences in the models themselves, rather than the priors.

Now let's find the posteriors of each and see how they vary.

### Updating $D_1$

Finding posterior via naive simulation
```{r}
# We sample from the prior a lot, predict with the param values from the prior,
# and keep if the prediction is equal to the observed data

df = data.frame(alpha = numeric(), beta = numeric())

i = 1
while (i <= pp_sample_size) {
  alpha = rlnorm(1, log(5), 0.01)
  beta = rlnorm(1, log(3), 0.01)
  
  y_pred = r_beta_binom(1, n_obs, alpha, beta)
  
  if (y_pred == y_obs) {
    df = rbind(df, data.frame(alpha=alpha, beta=beta))
    i = i+1
  }
}

df <- mutate(df, post_pred = r_beta_binom(n(), n_obs, alpha, beta))
```


### Updating $M$
This is a straightforward application of the Beta-Binomial Prior.

$$
Y \sim Binom(100, p), \quad p \sim Beta(5, 3)
$$

so
$$
p \mid Y = `r y_obs` \sim Beta\!\big(5+`r y_obs`,\ 3+`r n_obs-y_obs`\big)
$$

```{r}
post_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)
```


### Comparing analysis for $M$ vs equivalent analysis for $D_1$

```{r}
tvd_post = pp_sample_distance(df$post_pred, post_predictive_M)

plot_pp_difference(real_sample = df$post_pred, real_model_name = "D1",
                   miss_sample = post_predictive_M, miss_model_name = "M",
                   prior_or_post = "Posterior")
```

The posterior predictive distributions are quite different, with a Total Variational Distance of `r tvd_post` (a `r tvd_post/tvd_prior` time increase from the prior TVD), so the model misspecification in this case seems to have a significant impact on results.

## Example 2. Real data is dependent, but assumed to be independent.

Now, we explore what happens if our sample comes from a dependent model. Specifically, we generate a sample from summing a Markov Chain of $n$ drifting Bernoulli trials, denoted as model $D_2$.

$$
Y \sim MC(n=100, \ p_0=0.4, \ \delta=0.1)
$$
(MC for Markov Counts, details for sampling are in the helper functions section)

Where for $n$ trials, $p_0$ is the initial probability of success, and $\delta$ is the standard deviation of gaussian drift added to the success probability after each trial.

```{r}
n_ops = 100
y = r_markov_bernoulli(n_ops, p0=0.4, drift_sd=0.1)
y_obs = sum(y)
y_obs
```


Again, we will misspecify the model by assuming the data comes from a binomial distribution. We assign the same $\text{Beta}(5, 3)$ prior on $p$. Again, we call this model $M$.
$$
\begin{aligned}
Y &\sim \text{Binom}(n=100, p) \\
(Prior)\quad p &\sim \text{Beta}(5, 3)
\end{aligned}
$$

Let's then find an equivalent prior for model $D_2$.
$$
\begin{aligned}
Y &\sim MC(n=100, p_0, \delta) \\
(Prior)\quad p_0 &\sim \text{Beta}(\alpha, \beta) \\
(Prior)\quad \delta &\sim \text{Exp}(\lambda)
\end{aligned}
$$
We'll need a function to sample a prior predictive distribution for $D_2$.
```{r}
sample_from_pp_D2 = function(n, size, alpha, beta, rate) {
    p0 = rbeta(n, alpha, beta)
    drift_sd = rexp(n, rate)
    sample = r_markov_counts(n, size, p0, drift_sd)
    return(sample)
}
```

Let's use this prior distribution:
$$
\begin{aligned}
Y &\sim MC(n=100, \ p_0, \ \delta) \\
(Prior)\quad p_0 &\sim \text{Beta}(5, 3) \\
(Prior)\quad \delta &\sim \text{Exp}(100)
\end{aligned}
$$

And compare the prior predictive distributions:
```{r}
prior_pred_D2 = sample_from_pp_D2(n=pp_sample_size,
                                  size = n_obs,
                                  alpha=5, beta=3, rate=100)

tvd_prior = pp_sample_distance(prior_pred_D2, prior_pred_M)
tvd_prior

plot_pp_difference(real_sample = prior_pred_D2, real_model_name="D2",
                   miss_sample = prior_pred_M, miss_model_name="M",
                   prior_or_post = "Prior")
```

As shown, the priors are effectively equivalent.

Let's proceed with updating both models to see what kind of discrepancy there is.

### Updating $D_2$

We use naive simulation again to create a posterior sample for D2.
(This one is much slower, so we're also accepting if the predicted is within 1 of the observed, which makes the posterior a bit less accurate.)
```{r}
df = data.frame(p0 = numeric(), drift_sd = numeric())

i = 1
while (i <= pp_sample_size) {
  p0 = rbeta(1, 5, 3)
  drift_sd = rexp(1, 100)
  
  y_pred = r_markov_counts(1, n_obs, p0, drift_sd)
  
  if (abs(y_pred - y_obs) <= 1) {
    df = rbind(df, data.frame(p0=p0, drift_sd=drift_sd))
    i = i+1
  }
}

df <- mutate(df, post_pred = r_markov_counts(n(), n_obs, p0, drift_sd))
```

### Updating $M$
Again, a straightforward updating of the Beta-Binomial Prior.
```{r}
post_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)
```

### Comparing Posterior Predictive Distributions:
```{r}
tvd_post = pp_sample_distance(df$post_pred, post_predictive_M, method = 'tvd')

plot_pp_difference(real_sample = df$post_pred, real_model_name = "D2",
                   miss_sample = post_predictive_M, miss_model_name = "M",
                   prior_or_post = "Posterior")
```

There is a Total Variational Distance of `r tvd_post` (a `r tvd_post/tvd_prior` time increase from the prior TVD), and we can see from the plot, the posterior predictive distributions are visibly different.


## Conclusion

These examples demonstrate that model misspecification can have significant impacts of the results of a bayesian analyses. In terms of real world implications, these examples can serve as a cautionary tale to not blindly trust a model, even one with a reasonable prior predictive distribution.

### Future Work

An end goal of this project would be to analyze which statistics and tests are robust to varying degrees of model misspecification. To do this, we anticipate we'll need to figure out the following:

- Given a true underlying model, a method for quantifying or classifying how different (and therefore misspecified) a proposed model is.
- Given a true model and proposed model (+prior distribution), an efficient method for finding prior distributions for the true model such that their predictive distribution matches that of the propsed model. This would let us find effectively equivalent prior distributions, which we need to isolate the effects of model differences. (2)

(2) It has proven to be very difficult to find prior distributions such that they have some specified prior predictive distribution, because the search space of prior distributions is very very large. For future work, we could look for a more viable and general method of searching for priors.


## Appendix

### Helper Functions

Functions are sourced at the top so they stay out of the way while reading. Full helper code is shown here for reference.

```{r, echo=FALSE, results="asis"}
helper_lines <- readLines("helpers.R")
cat("```r\n", paste(helper_lines, collapse = "\n"), "\n```\n")
```

### Errata

We tried using grid search over parameterized priors to find priors with equivalent prior predictive distributions, but it did not work well at all.
```{r}
# grid = expand.grid(
#   mu1 = seq(1, 50, by=5),
#   mu2 = seq(1, 50, by=5),
#   sigma1 = seq(1, 50, by=5),
#   sigma2 = seq(1, 50, by=5)
# )

# grid = grid |> 
#   mutate(
#     pp_sample = pmap(
#       list(mu1, sigma1, mu2, sigma2),
#       ~ sample_from_pp_D1(n = pp_sample_size,
#                           size = n_obs,
#                           mu1 = ..1, sigma1 = ..2,
#                           mu2 = ..3, sigma2 = ..4))) |> 
#   mutate(
#     distances = map(pp_sample, ~ pp_sample_distance(.x, prior_pred_M))
#   ) |> 
#   arrange(distances)

```


We tried to use a different kind of markov model originally, but could not find priors which made equivalent prior predictive distributions for it.
```{r}
# (UNUSED because couldn't find equivalent predictive prior) function to sample Markov Bernoulli Distribution
r_markov_bernoulli_old = function(n, p_initial, p_after_1, p_after_0){
  sample = numeric(n)
  sample[1] = rbinom(1,1,p_initial)
  for (i in 2:n) {
    if (sample[i-1]==1) {sample[i] = rbinom(1,1, p_after_1)}
    else {sample[i] = rbinom(1,1, p_after_0)}
  }
  return(sample)
}

# (UNUSED because couldn't find equivalent predictive prior) function to sample counts from 'size'-number of markov bernoulli trials
r_markov_counts_old = function(n, size, p_initial, p_after_1, p_after_0) {
  sample = numeric(n)
  for (i in 1:n){ # for each sample, do 'size' bernoulli trials, then sum
    sample[i] = sum(r_markov_bernoulli(size, p_initial, p_after_1, p_after_0))
  }
  return(sample)
}
```
