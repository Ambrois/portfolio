[
  {
    "objectID": "posts/blackholesim/index.html",
    "href": "posts/blackholesim/index.html",
    "title": "Black Hole Visualizer",
    "section": "",
    "text": "Introduction\nTODO\n\nUsing C++, SFML, and GLSL\n\nmy first C++ and graphics project so please forgive strange and bad design choices\n\nThe simulation marches a grid of light rays (one per pixel) from the camera and numerically approximates the null geodesic equations for the Schwarzschild metric. The light rays curve and hit objects, which determine the color of the pixel.\n\nnumerical approximation is done via RK4 with adaptive step size\n\nFor efficiency of computation, and accurate due to spherical symmetry of the SC spacetime, null geodesics are computed in 2D on the equator and rotated into place in 3D.\n\n\n\nMilestone Screenshots\nTODO add screen recording\n\nAdded background stars\nMade disk color more realistic: calculate temp -&gt; approximate black body radiation -&gt; color\nPhoton ring visible \nObject bounds rendering \nGot light to curve coherently in 3D \nFigured out null geodesic equations \nBuilt euclidean engine first \nDrew a circle! \n\n\n\nNext Steps:\n\nthe center of the bh has trouble rendering bc parameterizing with phi, make step size also adjust for phi\nif you move, your camera turns because I’ve yet to implement parallel transport of the camera directions\naccretion disk\n\nshould be flared out bc intertia\nneeds to use intersections fully, the ray skipped segments are noticable"
  },
  {
    "objectID": "posts/bayesian/index.html",
    "href": "posts/bayesian/index.html",
    "title": "Analysis of Examples of Bayesian Model Misspecification",
    "section": "",
    "text": "The accuracy of Bayesian inference depends on how well the chosen model captures the underlying data-generating process. However, in practice, models are often misspecified, with assumptions violated in various degrees. This analysis looks at two examples of model misspecification, comparing predictive distributions from correctly specified models with those from simpler misspecified alternatives.\nTo facilitate accurate comparison, we match prior predictive distributions across models, which means that observed discrepancies in the posterior arise primarily due to model misspecification. To compare predictive distributions, we use total variation distance (TVD) and visualizalizations. We use naive rejection sampling for posterior sampling because we lack an explicit formula for likelihood for the dependent model, parameter dimensions are small, and for consistency across examples.\nTwo examples are considered:\n\nWhen the true data-generating process is hierarchical (Beta-Binomial).\nWhen the true process involves dependent observations modeled as a drifting Markov chain."
  },
  {
    "objectID": "posts/bayesian/index.html#example-1.-real-data-is-hierarchical",
    "href": "posts/bayesian/index.html#example-1.-real-data-is-hierarchical",
    "title": "Analysis of Examples of Bayesian Model Misspecification",
    "section": "Example 1. Real Data is Hierarchical",
    "text": "Example 1. Real Data is Hierarchical\nOur data y follows the following hierarchical distribution, which we’ll denote D1.\n\\[\nY \\sim Binom(100, p) \\\\\np \\sim Beta(3, 4)\n\\]\n\nn_obs = 100\ny = r_beta_bernoulli(n_obs, alpha = 3, 4)\ny_obs = sum(y)\ny_obs\n\n[1] 42\n\n\nNow, we will misspecify the model.\nWe now assume the data comes from a binomial distribution, and assign a beta(5,3) prior on the proportion, where Y is the sum of the data, We’ll call this model M (for “Misspecified”). \\[\nY \\sim Binom(n=100, p) \\\\\n(Prior) \\quad p \\sim Beta(5, 3)\n\\]\nPrior Predictive distribution:\n\npp_sample_size = 10000\n\n# the predictive distribution follows a beta-binomial\nprior_pred_M = r_beta_binom(pp_sample_size, n_obs, 5, 3)\n\nNow to find an equivalent prior for the real model\nThe real model (D1): \\[\nY \\sim Binom(n=100, p) \\\\\np \\sim Beta(\\alpha, \\beta) \\\\\n(Prior) \\quad \\alpha \\sim logN(\\mu_1, \\sigma_1) \\\\\n(Prior) \\quad \\beta \\sim logN(\\mu_2, \\sigma_2)\n\\]\nA function to sample from the prior predictive distribution:\n\n# function for sampling from prior predictive distrib of D2 model\nsample_from_pp_D1 = function(n, size, mu1, sigma1, mu2, sigma2) {\n  alphas = rlnorm(n, mu1, sigma1)\n  betas = rlnorm(n, mu2, sigma2)\n  probs = rbeta(n, alphas, betas)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nWe will use logN(log(5),0.01) and logN(log(3),0.01) as the priors for alpha and beta in model D1, it was chosen specifically to create a very similar prior predictive distribution to the prior for M.\nThese are the prior predictive distributions compared:\n\nprior_pred_D1 = sample_from_pp_D1(n = pp_sample_size,\n                                 size = n_obs,\n                                 mu1 = log(5), sigma1 = 0.01,\n                                 mu2 = log(3), sigma2 = 0.01)\n\ntvd_prior = pp_sample_distance(prior_pred_D1, prior_pred_M)\ntvd_prior\n\n[1] 0.0447\n\nplot_pp_difference(real_sample = prior_pred_D1, real_model_name = \"D1\",\n                   miss_sample = prior_pred_M, miss_model_name = \"M\",\n                   prior_or_post = \"Prior\")\n\n\n\n\n\n\n\n\nAs you can see, the predictive distributions for the two models are extremely similar, which suggests the two priors are effectively equivalent.\nThis implies that any differences observed in the posterior distributions after updating on data are likely attributable to differences in the models themselves, rather than the priors.\nNow let’s find the posteriors of each and see how they vary.\n\nUpdating D1\nFinding posterior via naive simulation\n\n# We sample from the prior a lot, predict with the param values from the prior, and keep if the prediction is equal to the observed data\n\ndf = data.frame(alpha = numeric(), beta = numeric())\n\ni = 1\nwhile (i &lt;= pp_sample_size) {\n  alpha = rlnorm(1, log(5), 0.01)\n  beta = rlnorm(1, log(3), 0.01)\n  \n  y_pred = r_beta_binom(1, n_obs, alpha, beta)\n  \n  if (y_pred == y_obs) {\n    df = rbind(df, data.frame(alpha=alpha, beta=beta))\n    i = i+1\n  }\n}\n\ndf &lt;- mutate(df, post_pred = r_beta_binom(n(), n_obs, alpha, beta))\n\n\n\nUpdating M\nThis is a very straightforward application of the Beta-Binomial Prior.\nY ~ binomial(100, p), with p ~ Beta(5,3)\nso p|Y=42` ~ Beta(5+42, 3+58)\n\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n\n\n\nComparing analysis for M vs equivalent analysis for D1\n\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M)\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D1\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n\n\n\n\n\n\n\n\nThe posterior predictive distributions are quite different, with a Total Variational Distance of 0.648 (a 14.4966443 time increase from the prior TVD), so the model misspecification in this case seems to have a significant impact on results."
  },
  {
    "objectID": "posts/bayesian/index.html#example-2.-real-data-is-dependent",
    "href": "posts/bayesian/index.html#example-2.-real-data-is-dependent",
    "title": "Analysis of Examples of Bayesian Model Misspecification",
    "section": "Example 2. Real Data is Dependent",
    "text": "Example 2. Real Data is Dependent\nNow, we explore what happens if our sample comes from a dependent model. Specifically, we generate a sample from summing a Markov Chain of \\(n\\) drifting Bernoulli trials, denoted as model D2.\n\\[\nY \\sim MC(n=100, \\ p_0=0.4, \\ \\delta=0.1)\n\\] (MC for Markov Counts, details for sampling are in the helper functions section)\nWhere for \\(n\\) trials, \\(p_0\\) is the initial probability of success, and \\(\\delta\\) is the standard deviation of gaussian drift added to the success probability after each trial.\n\nn_ops = 100\ny = r_markov_bernoulli(n_ops, p0=0.4, drift_sd=0.1)\ny_obs = sum(y)\ny_obs\n\n[1] 34\n\n\nAgain, we will misspecify the model by assuming the data comes from a binomial distribution. We assign the same beta(5,3) prior on \\(p\\). Again, we call this model M. \\[\nY \\sim Binom(n=100, p) \\\\\n(Prior) \\quad p \\sim Beta(5, 3)\n\\]\nLet’s then find an equivalent prior for model D2. \\[\nY \\sim MC(n=100, p_0, \\delta) \\\\\n(Prior) \\quad p_0 \\sim Beta(\\alpha, \\beta) \\\\\n(Prior) \\quad \\delta \\sim  Exp(\\lambda)\n\\] We’ll need a function to sample a prior predictive distribution for D2.\n\nsample_from_pp_D2 = function(n, size, alpha, beta, rate) {\n    p0 = rbeta(n, alpha, beta)\n    drift_sd = rexp(n, rate)\n    sample = r_markov_counts(n, size, p0, drift_sd)\n    return(sample)\n}\n\nLet’s use this prior distribution: \\[\nY \\sim MC(n=100, \\ p_0, \\ \\delta) \\\\\n(Prior) \\quad p_0 \\sim Beta(5, 3) \\\\\n(Prior) \\quad \\delta \\sim Exp(100)\n\\]\nAnd compare the prior predictive distributions:\n\nprior_pred_D2 = sample_from_pp_D2(n=pp_sample_size,\n                                  size = n_obs,\n                                  alpha=5, beta=3, rate=100)\n\ntvd_prior = pp_sample_distance(prior_pred_D2, prior_pred_M)\ntvd_prior\n\n[1] 0.0601\n\nplot_pp_difference(real_sample = prior_pred_D2, real_model_name=\"D2\",\n                   miss_sample = prior_pred_M, miss_model_name=\"M\",\n                   prior_or_post = \"Prior\")\n\n\n\n\n\n\n\n\nAs shown, the priors are effectively equivalent.\nLet’s proceed with updating both models to see what kind of discrepancy there is.\n\nUpdating D2\nWe use naive simulation again to create a posterior sample for D2. (This one is much slower, so we’re also accepting if the predicted is within 1 of the observed, which makes the posterior a bit less accurate.)\n\ndf = data.frame(p0 = numeric(), drift_sd = numeric())\n\ni = 1\nwhile (i &lt;= pp_sample_size) {\n  p0 = rbeta(1, 5, 3)\n  drift_sd = rexp(1, 100)\n  \n  y_pred = r_markov_counts(1, n_obs, p0, drift_sd)\n  \n  if (abs(y_pred - y_obs) &lt;= 1) {\n    df = rbind(df, data.frame(p0=p0, drift_sd=drift_sd))\n    i = i+1\n  }\n}\n\ndf &lt;- mutate(df, post_pred = r_markov_counts(n(), n_obs, p0, drift_sd))\n\n\n\nUpdating M\nAgain, a straightforward updating of the Beta-Binomial Prior.\n\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n\n\n\nComparing Posterior Predictive Distributions:\n\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M, method = 'tvd')\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D2\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n\n\n\n\n\n\n\n\nThere is a Total Variational Distance of 0.214 (a 3.5607321 time increase from the prior TVD), and we can see from the plot, the posterior predictive distributions are visibly different."
  },
  {
    "objectID": "posts/bayesian/index.html#conclusion",
    "href": "posts/bayesian/index.html#conclusion",
    "title": "Analysis of Examples of Bayesian Model Misspecification",
    "section": "Conclusion",
    "text": "Conclusion\nThese examples demonstrate that model misspecification can have significant impacts of the results of a bayesian analyses. In terms of real world implications, it’s unclear how realistic the examples we’ve used are, but it can still serve as a cautionary tale to not blindly trust your model.\nIt has proven to be very difficult to find prior distributions such that they have some specified prior predictive distribution. For future work, we could look for a more viable and general method of searching for priors."
  },
  {
    "objectID": "posts/bayesian/index.html#helper-functions-appendix",
    "href": "posts/bayesian/index.html#helper-functions-appendix",
    "title": "Analysis of Examples of Bayesian Model Misspecification",
    "section": "Helper Functions (appendix)",
    "text": "Helper Functions (appendix)\nFunctions are sourced at the top so they stay out of the way while reading. Full helper code is shown here for reference.\n r_beta_bernoulli = function(n, alpha, beta) {\n  probs = rbeta(n, alpha, beta)\n  sample = rbinom(n, 1, probs)\n  return(sample)\n}\n\nr_beta_binom = function(n, size, alpha, beta) {\n  probs = rbeta(n, alpha, beta)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nr_markov_bernoulli = function(n, p0, drift_sd) {\n  probs = numeric(n)\n  probs[1] = p0\n  \n  for (i in 2:n) {\n    probs[i] = probs[i - 1] + rnorm(1, 0, drift_sd)\n    probs[i] = min(max(probs[i], 0), 1)\n  }\n  sample = rbinom(n, 1, probs)\n  return(sample)\n}\n\nr_markov_counts = function(n, size, p0, drift_sd) {\n  sample = numeric(n)\n  for (i in 1:n) {\n    sample[i] = sum(r_markov_bernoulli(size, p0[i], drift_sd[i]))\n  }\n  return(sample)\n}\n\npp_sample_distance = function(pp_sample1, pp_sample2, method = \"tvd\") {\n  \n  if (length(pp_sample1) != length(pp_sample2)) {\n    print(\"warning: sample sizes are different\")\n  }\n  \n  if (method == \"tvd\") {\n    max_val = max(c(pp_sample1, pp_sample2))\n    \n    counts1 = tabulate(pp_sample1, nbins = max_val)\n    counts2 = tabulate(pp_sample2, nbins = max_val)\n    \n    counts1 = counts1 / sum(counts1)\n    counts2 = counts2 / sum(counts2)\n    \n    return(0.5 * sum(abs(counts1 - counts2)))\n  } else if (method == \"binned_tvd\") {\n    combined = c(pp_sample1, pp_sample2)\n    breaks = hist(combined, plot = FALSE)$breaks\n  \n    counts1 = hist(pp_sample1, breaks = breaks, plot = FALSE)$counts\n    counts2 = hist(pp_sample2, breaks = breaks, plot = FALSE)$counts\n    \n    counts1 = counts1 / sum(counts1)\n    counts2 = counts2 / sum(counts2)\n    \n    return(0.5 * sum(abs(counts1 - counts2)))\n  } else {\n    print(\"available methods are 'tvd' and 'binned_tcd'\")\n  }\n}\n\nplot_pp_difference = function(real_sample, miss_sample, prior_or_post, real_model_name, miss_model_name) {\n  \n  plot_df = tibble(\n   D = real_sample,\n   M = miss_sample\n  ) |&gt; \n  pivot_longer(\n    cols = D:M,\n    names_to = \"source\",\n    values_to = \"values\"\n  )\n\n  ggplot(plot_df, aes(x = values, fill = source)) +\n    geom_histogram(aes(y = after_stat(density)), position = \"identity\", alpha = 0.5, bins = 30) +\n    labs(title = glue(\"{prior_or_post} Predictive Distribution Comparison\"), x = \"Count\", y = \"Density\") +\n    scale_fill_discrete(\n      labels = c(glue(\"Real Model ({real_model_name})\"), glue(\"Misspecified Model ({miss_model_name})\"))) + \n    theme_minimal()\n}\n\nsample_from_pp_D1 = function(n, size, mu1, sigma1, mu2, sigma2) {\n  alphas = rlnorm(n, mu1, sigma1)\n  betas = rlnorm(n, mu2, sigma2)\n  probs = rbeta(n, alphas, betas)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nsample_from_pp_D2 = function(n, size, alpha, beta, rate) {\n    p0 = rbeta(n, alpha, beta)\n    drift_sd = rexp(n, rate)\n    sample = r_markov_counts(n, size, p0, drift_sd)\n    return(sample)\n}\n\nr_markov_bernoulli_old = function(n, p_initial, p_after_1, p_after_0){\n  sample = numeric(n)\n  sample[1] = rbinom(1, 1, p_initial)\n  for (i in 2:n) {\n    if (sample[i - 1] == 1) {sample[i] = rbinom(1, 1, p_after_1)}\n    else {sample[i] = rbinom(1, 1, p_after_0)}\n  }\n  return(sample)\n}\n\nr_markov_counts_old = function(n, size, p_initial, p_after_1, p_after_0) {\n  sample = numeric(n)\n  for (i in 1:n) {\n    sample[i] = sum(r_markov_bernoulli_old(size, p_initial, p_after_1, p_after_0))\n  }\n  return(sample)\n}"
  },
  {
    "objectID": "posts/bayesian/index.html#errata",
    "href": "posts/bayesian/index.html#errata",
    "title": "Analysis of Examples of Bayesian Model Misspecification",
    "section": "Errata",
    "text": "Errata\nWe tried using grid search to find priors with equivalent prior predictive distributions, but it did not work well at all.\n\n# grid = expand.grid(\n#   mu1 = seq(1, 50, by=5),\n#   mu2 = seq(1, 50, by=5),\n#   sigma1 = seq(1, 50, by=5),\n#   sigma2 = seq(1, 50, by=5)\n# )\n\n# grid = grid |&gt; \n#   mutate(\n#     pp_sample = pmap(\n#       list(mu1, sigma1, mu2, sigma2),\n#       ~ sample_from_pp_D1(n = pp_sample_size,\n#                           size = n_obs,\n#                           mu1 = ..1, sigma1 = ..2,\n#                           mu2 = ..3, sigma2 = ..4))) |&gt; \n#   mutate(\n#     distances = map(pp_sample, ~ pp_sample_distance(.x, prior_pred_M))\n#   ) |&gt; \n#   arrange(distances)\n\nWe tried to use a different kind of markov model originally, but could not find priors which made equivalent prior predictive distributions for it.\n\n# (UNUSED because couldn't find equivalent predictive prior) function to sample Markov Bernoulli Distribution\nr_markov_bernoulli_old = function(n, p_initial, p_after_1, p_after_0){\n  sample = numeric(n)\n  sample[1] = rbinom(1,1,p_initial)\n  for (i in 2:n) {\n    if (sample[i-1]==1) {sample[i] = rbinom(1,1, p_after_1)}\n    else {sample[i] = rbinom(1,1, p_after_0)}\n  }\n  return(sample)\n}\n\n# (UNUSED because couldn't find equivalent predictive prior) function to sample counts from 'size'-number of markov bernoulli trials\nr_markov_counts_old = function(n, size, p_initial, p_after_1, p_after_0) {\n  sample = numeric(n)\n  for (i in 1:n){ # for each sample, do 'size' bernoulli trials, then sum\n    sample[i] = sum(r_markov_bernoulli(size, p_initial, p_after_1, p_after_0))\n  }\n  return(sample)\n}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Close this page right now."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "Close this page right now."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew Huang",
    "section": "",
    "text": "Welcome to my portfolio"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Black Hole Visualizer\n\n\nBlack hole raymarcher from scratch, using C++, SFML, and GLSL\n\n\n\n\n\nNov 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Examples of Bayesian Model Misspecification\n\n\nComparing bayesian predictive distributions under examples of model misspecification.\n\n\n\n\n\nJun 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNarrative Visualization (Scratch)\n\n\n\n\n\n\n\n\nJun 2, 2025\n\n\nMatthew Huang\n\n\n\n\n\n\n\n\n\n\n\n\nTransformer Notes (Draft)\n\n\n\n\n\n\n\n\nJun 2, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Stat365/index.html",
    "href": "posts/Stat365/index.html",
    "title": "Narrative Visualization (Scratch)",
    "section": "",
    "text": "counts_male &lt;- age_gaps |&gt; \n  filter(character_1_gender == \"man\") |&gt; \n  count(age_difference)\ncounts_female &lt;- age_gaps |&gt; \n  filter(character_1_gender == \"woman\") |&gt; \n  count(age_difference)\n\nggplot() +\n  geom_col(data = counts_male, aes(x = age_difference, y = n), fill = \"steelblue\", width=1) +\n  geom_col(data = counts_female, aes(x = age_difference, y = -n), fill = \"pink\", width=1) +\n  geom_label(aes(x=30, y=40, label=\"Male\"), color=\"steelblue\")+\n  geom_label(aes(x=30, y=-20, label=\"Female\"), color=\"pink\")+\n  labs(title = \"Age Differences in Movie Couples\",\n       subtitle = \"By Gender of the Older Actor\",\n       x='', y='',\n       caption = \"Data sourced from hollywoodagegap.com.\") +\n  theme_minimal()\n\n\n\n\nFigure 1.2 shows the age differences in holywood movie couples. It’s clear that male older relationships are significantly more common than female older relationships, as well as having larger average age gaps.\n\n\n\n\n\nage_gaps &lt;- age_gaps |&gt;\n  mutate(maleFemDiff = case_when(\n    character_1_gender == \"woman\" ~ -1 * age_difference,\n    character_1_gender == \"man\" ~ age_difference\n  ))\n\nggplot() +\n  geom_histogram(data = age_gaps %&gt;% filter(maleFemDiff &gt;= 0), aes(x = maleFemDiff), fill = \"steelblue\", binwidth = 1) +\n  geom_histogram(data = age_gaps %&gt;% filter(maleFemDiff &lt; 0), aes(x = maleFemDiff), fill = \"pink\", binwidth = 1) +\n  geom_label(aes(x =20, y = 35, label = \"Male\"), color = \"steelblue\") +\n  geom_label(aes(x = -20, y = 35, label = \"Female\"), color = \"pink\")+\n    labs(\n    title = \"Age Differences in Heterosexual Movie Couples\",\n    subtitle = \"By age of Male character - Female character\",\n    y=\"Count\",\n    x=\"Male age - Female age\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# normality check:\nqqnorm(age_gaps$maleFemDiff)\nqqline(age_gaps$maleFemDiff)\n\n\n\n\n\n\n\n# t.test\nt.test(age_gaps$maleFemDiff, mu=0, alternative=\"greater\")\n\n\n    One Sample t-test\n\ndata:  age_gaps$maleFemDiff\nt = 27.434, df = 1154, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 7.946419      Inf\nsample estimates:\nmean of x \n  8.45368"
  },
  {
    "objectID": "posts/transformers/index.html",
    "href": "posts/transformers/index.html",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "Transformer Circuits Thread (transformer-circuits.pub)\nTESTING EDIT\n\n\nprivileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -\n\n\n\nresidual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear\n\n\n\n\n\nT(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)\n\n\n\n\n\n\n“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  },
  {
    "objectID": "posts/transformers/index.html#things-i-dont-get",
    "href": "posts/transformers/index.html#things-i-dont-get",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "privileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -"
  },
  {
    "objectID": "posts/transformers/index.html#notes",
    "href": "posts/transformers/index.html#notes",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "residual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear"
  },
  {
    "objectID": "posts/transformers/index.html#basic-definitions",
    "href": "posts/transformers/index.html#basic-definitions",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "T(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)"
  },
  {
    "objectID": "posts/transformers/index.html#notes-1",
    "href": "posts/transformers/index.html#notes-1",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  }
]