[
  {
    "objectID": "programming.html",
    "href": "programming.html",
    "title": "Programming Projects",
    "section": "",
    "text": "Programming-heavy work and experiments.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects_archived/transformers/index.html",
    "href": "projects_archived/transformers/index.html",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "Transformer Circuits Thread (transformer-circuits.pub)\nTESTING EDIT\n\n\nprivileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -\n\n\n\nresidual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear\n\n\n\n\n\nT(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)\n\n\n\n\n\n\n“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  },
  {
    "objectID": "projects_archived/transformers/index.html#things-i-dont-get",
    "href": "projects_archived/transformers/index.html#things-i-dont-get",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "privileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -"
  },
  {
    "objectID": "projects_archived/transformers/index.html#notes",
    "href": "projects_archived/transformers/index.html#notes",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "residual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear"
  },
  {
    "objectID": "projects_archived/transformers/index.html#basic-definitions",
    "href": "projects_archived/transformers/index.html#basic-definitions",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "T(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)"
  },
  {
    "objectID": "projects_archived/transformers/index.html#notes-1",
    "href": "projects_archived/transformers/index.html#notes-1",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew Huang",
    "section": "",
    "text": "I am a 2025 graduate from Cal Poly SLO with a bachelor’s in statistics and minors in math and computer science. I am interested in math and sciences of all sorts, but especially modeling and it’s limitiations.\nCurrently open to work.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Matthew Huang",
    "section": "Projects",
    "text": "Projects",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "projects/blackholesim/index.html",
    "href": "projects/blackholesim/index.html",
    "title": "Black Hole Simulation",
    "section": "",
    "text": "This project is a real-time black hole visualization engine built from scratch in C++, using GPU shaders for parallel computation. The goal was to simulate how light bends near a black hole and render the resulting image on screen, while simultaneously modern C++, graphics programming, and numerical simulation techniques.\nAt a high level, the program treats each screen pixel as a ray of light emitted from a virtual camera. These rays are numerically simulated as they travel through curved spacetime near a black hole. Depending on how each ray bends and what it intersects (background stars or an accretion disk), the program computes the final color of that pixel. The entire process runs in parallel on the GPU, allowing thousands of light paths to be simulated simultaneously.\nThis project required building multiple systems end-to-end:\n\nA custom ray-marching pipeline implemented in GLSL\nA numerical solver (fourth-order Runge–Kutta with adaptive step size) for physically motivated differential equations\nA rendering pipeline integrating C++, SFML, and shaders\nPerformance optimizations and debugging in a GPU-only environment\n\nAlthough the underlying physics is based on general relativity, the primary focus of the project was systems design, numerical methods, and performance-conscious implementation, rather than producing a scientifically exact model. This was my first C++ and graphics project, and it served as a practical way to learn lower-level programming, GPU parallelism, and how complex simulations are built and debugged in practice.\nView Repository",
    "crumbs": [
      "Projects",
      "Black Hole Simulation"
    ]
  },
  {
    "objectID": "projects/blackholesim/index.html#introduction",
    "href": "projects/blackholesim/index.html#introduction",
    "title": "Black Hole Simulation",
    "section": "",
    "text": "This project is a real-time black hole visualization engine built from scratch in C++, using GPU shaders for parallel computation. The goal was to simulate how light bends near a black hole and render the resulting image on screen, while simultaneously modern C++, graphics programming, and numerical simulation techniques.\nAt a high level, the program treats each screen pixel as a ray of light emitted from a virtual camera. These rays are numerically simulated as they travel through curved spacetime near a black hole. Depending on how each ray bends and what it intersects (background stars or an accretion disk), the program computes the final color of that pixel. The entire process runs in parallel on the GPU, allowing thousands of light paths to be simulated simultaneously.\nThis project required building multiple systems end-to-end:\n\nA custom ray-marching pipeline implemented in GLSL\nA numerical solver (fourth-order Runge–Kutta with adaptive step size) for physically motivated differential equations\nA rendering pipeline integrating C++, SFML, and shaders\nPerformance optimizations and debugging in a GPU-only environment\n\nAlthough the underlying physics is based on general relativity, the primary focus of the project was systems design, numerical methods, and performance-conscious implementation, rather than producing a scientifically exact model. This was my first C++ and graphics project, and it served as a practical way to learn lower-level programming, GPU parallelism, and how complex simulations are built and debugged in practice.\nView Repository",
    "crumbs": [
      "Projects",
      "Black Hole Simulation"
    ]
  },
  {
    "objectID": "projects/blackholesim/index.html#progress-pictures",
    "href": "projects/blackholesim/index.html#progress-pictures",
    "title": "Black Hole Simulation",
    "section": "Progress pictures",
    "text": "Progress pictures\n \n\nAdded background stars\nMade disk color more realistic: calculate temperature -&gt; approximate black body radiation -&gt; color\nMore realistic disk sizing, note that the photon ring is now visible! I’m not sure what that very thin ring inside the photon ring is, and it might be a bug, but that’s for future me to figure out.\n\n\n - Added adaptive step sizing and other optimizations - Fixed object edge rendering for arbitrary step size - Added prettier colors and a grid to see how spacetime curves, and it’s very clearly showing gravitational lensing\n\n\n\n\nOlder Screenshot\n\n\n\nImplementing RK4 for the null geodesic equations in 3D.\nParallelized everything via fragment shader, which was quite difficult, as debugging information could only be encoded through pixel color. I’ve since learned that I should have used a compute shader and passed to a simple fragment shader, which would have been easier to debug and manage.\n\n\n\n\n\nOlder Screenshot\n\n\n\nDerived the null geodesic equations in 2D via Lagrangian, here I learned the basics of general relativity.\nImplemented RK4 for numerically solving said equations.\n\n\n\n\n\nOlder Screenshot\n\n\n\nBuilt basic euclidean raymarching engine.\nMore of learning how to program in C++.\n\n\n\n\n\nOlder Screenshot\n\n\n\nDrew a circle!\nLearned the very basics of C++, SFML, and GLSL.\n\n\n\nContext\nI started this project independently to learn C++, shader programming, and touch on general relativity. It took about a month and is so far the most difficult project I’ve mostly completed. I plan to develop more but am really tired of thinking about this project for now.\n\nI learned C++ with this free book.\nI was able to learn some general relativity with Sean Carrol’s Spacetime and Geometry, which can be acquired through allegedly legal means.\nI was inspired to start this project by this cool video. I did not look at any of his code, and used a different derivation for the geodesic equations. I was also more generally inspired by Sebastian Lague’s very cool youtube channel to try shader programming."
  },
  {
    "objectID": "projects/blackholesim/index.html#future-work",
    "href": "projects/blackholesim/index.html#future-work",
    "title": "Black Hole Simulation",
    "section": "Future work:",
    "text": "Future work:\n\nA more detailed writeup of the equation derivations and algorithm details is in process.\nThe center of the bh has trouble rendering bc parameterizing with phi, I’ll try to minimize this by adjusting the dynamic step size to account for starting angle\nIf you move, the camera turns slightly on its own, because I’ve yet to implement parallel transport of the camera directions, but this is in progress.\nAccretion disk adjustments:\n\nCould use better boundary detection.\nIts shape should be flared out a little for more realism.\nCould add doppler effect coloring.",
    "crumbs": [
      "Projects",
      "Black Hole Simulation"
    ]
  },
  {
    "objectID": "projects/bayesian/index.html",
    "href": "projects/bayesian/index.html",
    "title": "Analysis of Bayesian Model Misspecification",
    "section": "",
    "text": "In data analysis, models are never exactly correct. Simplifying assumptions, such as independence, homogeneity, or a specific distributional form, keep models tractable. While Bayesian methods are often viewed as robust because they incorporate uncertainty explicitly, they are still sensitive to model misspecification: where assumed model structure does not match the true data-generating process.\nThis project investigates how small, realistic forms of model misspecification can materially change Bayesian conclusions, even when prior assumptions appear reasonable. Rather than focusing on parameter estimates alone, we compare posterior predictive distributions, which directly reflects what the model believes future or unseen data will look like. This makes the analysis especially relevant for applied settings where Bayesian models are used to support forecasting, decision-making, or risk assessment.\nTo isolate the effect of misspecification, we construct pairs of models where:\n\nOne model matches the true data-generating process.\nThe other is a simpler, commonly used alternative.\nBoth models are calibrated to have nearly identical prior predictive behavior.\n\nBy matching prior predictive distributions, any divergence observed after seeing data can be attributed primarily to differences in model structure rather than differences in prior assumptions. We quantify these differences using total variation distance and visualize how predictive beliefs diverge after updating on the same observations.\nTwo scenarios are examined:\n\nData generated from a hierarchical process but analyzed using a non-hierarchical model.\nData with dependence between observations but analyzed under an independence assumption.\n\nAcross both cases, we find that seemingly mild misspecification can lead to substantially different posterior predictions. The results highlight an important lesson for applied Bayesian modeling: a model that appears well-calibrated before seeing data may still produce misleading inferences if its structural assumptions are wrong.",
    "crumbs": [
      "Projects",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "projects/bayesian/index.html#introduction",
    "href": "projects/bayesian/index.html#introduction",
    "title": "Analysis of Bayesian Model Misspecification",
    "section": "",
    "text": "In data analysis, models are never exactly correct. Simplifying assumptions, such as independence, homogeneity, or a specific distributional form, keep models tractable. While Bayesian methods are often viewed as robust because they incorporate uncertainty explicitly, they are still sensitive to model misspecification: where assumed model structure does not match the true data-generating process.\nThis project investigates how small, realistic forms of model misspecification can materially change Bayesian conclusions, even when prior assumptions appear reasonable. Rather than focusing on parameter estimates alone, we compare posterior predictive distributions, which directly reflects what the model believes future or unseen data will look like. This makes the analysis especially relevant for applied settings where Bayesian models are used to support forecasting, decision-making, or risk assessment.\nTo isolate the effect of misspecification, we construct pairs of models where:\n\nOne model matches the true data-generating process.\nThe other is a simpler, commonly used alternative.\nBoth models are calibrated to have nearly identical prior predictive behavior.\n\nBy matching prior predictive distributions, any divergence observed after seeing data can be attributed primarily to differences in model structure rather than differences in prior assumptions. We quantify these differences using total variation distance and visualize how predictive beliefs diverge after updating on the same observations.\nTwo scenarios are examined:\n\nData generated from a hierarchical process but analyzed using a non-hierarchical model.\nData with dependence between observations but analyzed under an independence assumption.\n\nAcross both cases, we find that seemingly mild misspecification can lead to substantially different posterior predictions. The results highlight an important lesson for applied Bayesian modeling: a model that appears well-calibrated before seeing data may still produce misleading inferences if its structural assumptions are wrong.",
    "crumbs": [
      "Projects",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "projects/bayesian/index.html#example-1.-real-data-is-hierarchical-but-we-assume-flat",
    "href": "projects/bayesian/index.html#example-1.-real-data-is-hierarchical-but-we-assume-flat",
    "title": "Analysis of Bayesian Model Misspecification",
    "section": "Example 1. Real data is hierarchical, but we assume flat",
    "text": "Example 1. Real data is hierarchical, but we assume flat\nOur data \\(y\\) follows the following hierarchical distribution, which we’ll denote \\(D_1\\).\n\\[\n\\begin{align}\nY &\\sim Binom(100, p) \\\\\np &\\sim Beta(3, 4)\n\\end{align}\n\\]\n\nn_obs = 100\ny = r_beta_bernoulli(n_obs, alpha = 3, 4)\ny_obs = sum(y)\ny_obs\n\n[1] 42\n\n\nNow, we will misspecify the model.\nWe now assume the data comes from a binomial distribution, and assign a beta(5,3) prior on the proportion, where Y is the sum of the data, We’ll call this model \\(M\\) (for “Misspecified”). \\[\n\\begin{aligned}\nY &\\sim \\text{Binom}(n = 100, p) \\\\\n(Prior)\\quad p &\\sim \\text{Beta}(5, 3)\n\\end{aligned}\n\\]\nPrior Predictive distribution:\n\npp_sample_size = 10000\n\n# the predictive distribution follows a beta-binomial\nprior_pred_M = r_beta_binom(pp_sample_size, n_obs, 5, 3)\n\nNow to find an equivalent prior for the real model\nThe real model (\\(D_1\\)): \\[\n\\begin{aligned}\nY &\\sim \\text{Binom}(n=100, p) \\\\\np &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n(Prior)\\quad \\alpha &\\sim \\log \\mathcal{N}(\\mu_1, \\sigma_1) \\\\\n(Prior)\\quad \\beta &\\sim \\log \\mathcal{N}(\\mu_2, \\sigma_2)\n\\end{aligned}\n\\]\nA function to sample from the prior predictive distribution:\n\n# function for sampling from prior predictive distrib of D2 model\nsample_from_pp_D1 = function(n, size, mu1, sigma1, mu2, sigma2) {\n  alphas = rlnorm(n, mu1, sigma1)\n  betas = rlnorm(n, mu2, sigma2)\n  probs = rbeta(n, alphas, betas)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nWe will use lognormal \\(\\log \\mathcal{N}(\\log 5,\\ 0.01)\\) and \\(\\log \\mathcal{N}(\\log 3,\\ 0.01)\\) as the priors for \\(\\alpha\\) and \\(\\beta\\) in model \\(D_1\\), chosen to create a very similar prior predictive distribution to the prior for \\(M\\).\nThese are the prior predictive distributions compared:\n\nprior_pred_D1 = sample_from_pp_D1(n = pp_sample_size,\n                                 size = n_obs,\n                                 mu1 = log(5), sigma1 = 0.01,\n                                 mu2 = log(3), sigma2 = 0.01)\n\ntvd_prior = pp_sample_distance(prior_pred_D1, prior_pred_M)\ntvd_prior\n\n[1] 0.0447\n\nplot_pp_difference(real_sample = prior_pred_D1, real_model_name = \"D1\",\n                   miss_sample = prior_pred_M, miss_model_name = \"M\",\n                   prior_or_post = \"Prior\")\n\n\n\n\n\n\n\n\nAs you can see, the predictive distributions for the two models are extremely similar, which suggests the two priors are effectively equivalent.\nThis implies that any differences observed in the posterior distributions after updating on data are likely attributable to differences in the models themselves, rather than the priors.\nNow let’s find the posteriors of each and see how they vary.\n\nUpdating \\(D_1\\)\nFinding posterior via naive simulation\n\n# We sample from the prior a lot, predict with the param values from the prior,\n# and keep if the prediction is equal to the observed data\n\ndf = data.frame(alpha = numeric(), beta = numeric())\n\ni = 1\nwhile (i &lt;= pp_sample_size) {\n  alpha = rlnorm(1, log(5), 0.01)\n  beta = rlnorm(1, log(3), 0.01)\n  \n  y_pred = r_beta_binom(1, n_obs, alpha, beta)\n  \n  if (y_pred == y_obs) {\n    df = rbind(df, data.frame(alpha=alpha, beta=beta))\n    i = i+1\n  }\n}\n\ndf &lt;- mutate(df, post_pred = r_beta_binom(n(), n_obs, alpha, beta))\n\n\n\nUpdating \\(M\\)\nThis is a straightforward application of the Beta-Binomial Prior.\n\\[\nY \\sim Binom(100, p), \\quad p \\sim Beta(5, 3)\n\\]\nso \\[\np \\mid Y = 42 \\sim Beta\\!\\big(5+42,\\ 3+58\\big)\n\\]\n\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n\n\n\nComparing analysis for \\(M\\) vs equivalent analysis for \\(D_1\\)\n\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M)\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D1\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n\n\n\n\n\n\n\n\nThe posterior predictive distributions are quite different, with a Total Variational Distance of 0.648 (a 14.4966443 time increase from the prior TVD), so the model misspecification in this case seems to have a significant impact on results."
  },
  {
    "objectID": "projects/bayesian/index.html#example-2.-real-data-is-dependent-but-we-assume-independent",
    "href": "projects/bayesian/index.html#example-2.-real-data-is-dependent-but-we-assume-independent",
    "title": "Analysis of Bayesian Model Misspecification",
    "section": "Example 2. Real data is dependent, but we assume independent",
    "text": "Example 2. Real data is dependent, but we assume independent\nNow, we explore what happens if our sample comes from a dependent model. Specifically, we generate a sample from summing a Markov Chain of \\(n\\) drifting Bernoulli trials, denoted as model \\(D_2\\).\n\\[\nY \\sim MC(n=100, \\ p_0=0.4, \\ \\delta=0.1)\n\\] (MC for Markov Counts, details for sampling are in the helper functions section)\nWhere for \\(n\\) trials, \\(p_0\\) is the initial probability of success, and \\(\\delta\\) is the standard deviation of gaussian drift added to the success probability after each trial.\n\nn_ops = 100\ny = r_markov_bernoulli(n_ops, p0=0.4, drift_sd=0.1)\ny_obs = sum(y)\ny_obs\n\n[1] 34\n\n\nAgain, we will misspecify the model by assuming the data comes from a binomial distribution. We assign the same \\(\\text{Beta}(5, 3)\\) prior on \\(p\\). Again, we call this model \\(M\\). \\[\n\\begin{aligned}\nY &\\sim \\text{Binom}(n=100, p) \\\\\n(Prior)\\quad p &\\sim \\text{Beta}(5, 3)\n\\end{aligned}\n\\]\nLet’s then find an equivalent prior for model \\(D_2\\). \\[\n\\begin{aligned}\nY &\\sim MC(n=100, p_0, \\delta) \\\\\n(Prior)\\quad p_0 &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n(Prior)\\quad \\delta &\\sim \\text{Exp}(\\lambda)\n\\end{aligned}\n\\] We’ll need a function to sample a prior predictive distribution for \\(D_2\\).\n\nsample_from_pp_D2 = function(n, size, alpha, beta, rate) {\n    p0 = rbeta(n, alpha, beta)\n    drift_sd = rexp(n, rate)\n    sample = r_markov_counts(n, size, p0, drift_sd)\n    return(sample)\n}\n\nLet’s use this prior distribution: \\[\n\\begin{aligned}\nY &\\sim MC(n=100, \\ p_0, \\ \\delta) \\\\\n(Prior)\\quad p_0 &\\sim \\text{Beta}(5, 3) \\\\\n(Prior)\\quad \\delta &\\sim \\text{Exp}(100)\n\\end{aligned}\n\\]\nAnd compare the prior predictive distributions:\n\nprior_pred_D2 = sample_from_pp_D2(n=pp_sample_size,\n                                  size = n_obs,\n                                  alpha=5, beta=3, rate=100)\n\ntvd_prior = pp_sample_distance(prior_pred_D2, prior_pred_M)\ntvd_prior\n\n[1] 0.0601\n\nplot_pp_difference(real_sample = prior_pred_D2, real_model_name=\"D2\",\n                   miss_sample = prior_pred_M, miss_model_name=\"M\",\n                   prior_or_post = \"Prior\")\n\n\n\n\n\n\n\n\nAs shown, the priors are effectively equivalent.\nLet’s proceed with updating both models to see what kind of discrepancy there is.\n\nUpdating \\(D_2\\)\nWe use naive simulation again to create a posterior sample for D2. (This one is much slower, so we’re also accepting if the predicted is within 1 of the observed, which makes the posterior a bit less accurate.)\n\ndf = data.frame(p0 = numeric(), drift_sd = numeric())\n\ni = 1\nwhile (i &lt;= pp_sample_size) {\n  p0 = rbeta(1, 5, 3)\n  drift_sd = rexp(1, 100)\n  \n  y_pred = r_markov_counts(1, n_obs, p0, drift_sd)\n  \n  if (abs(y_pred - y_obs) &lt;= 1) {\n    df = rbind(df, data.frame(p0=p0, drift_sd=drift_sd))\n    i = i+1\n  }\n}\n\ndf &lt;- mutate(df, post_pred = r_markov_counts(n(), n_obs, p0, drift_sd))\n\n\n\nUpdating \\(M\\)\nAgain, a straightforward updating of the Beta-Binomial Prior.\n\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n\n\n\nComparing Posterior Predictive Distributions:\n\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M, method = 'tvd')\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D2\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n\n\n\n\n\n\n\n\nThere is a Total Variational Distance of 0.214 (a 3.5607321 time increase from the prior TVD), and we can see from the plot, the posterior predictive distributions are visibly different."
  },
  {
    "objectID": "projects/bayesian/index.html#conclusion",
    "href": "projects/bayesian/index.html#conclusion",
    "title": "Analysis of Bayesian Model Misspecification",
    "section": "Conclusion",
    "text": "Conclusion\nThese examples demonstrate that model misspecification can have significant impacts of the results of a bayesian analyses. In terms of real world implications, these examples can serve as a cautionary tale to not blindly trust a model, even one with a reasonable prior predictive distribution.\n\nFuture Work\nAn end goal of this project would be to analyze which statistics and tests are robust to varying degrees of model misspecification. To do this, we anticipate we’ll need to figure out the following:\n\nGiven a true underlying model, a method for quantifying or classifying how different (and therefore misspecified) a proposed model is.\nGiven a true model and proposed model (+prior distribution), an efficient method for finding prior distributions for the true model such that their predictive distribution matches that of the propsed model. This would let us find effectively equivalent prior distributions, which we need to isolate the effects of model differences. (2)\n\n\nIt has proven to be very difficult to find prior distributions such that they have some specified prior predictive distribution, because the search space of prior distributions is very very large. For future work, we could look for a more viable and general method of searching for priors.",
    "crumbs": [
      "Projects",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "projects/bayesian/index.html#appendix",
    "href": "projects/bayesian/index.html#appendix",
    "title": "Analysis of Bayesian Model Misspecification",
    "section": "Appendix",
    "text": "Appendix\n\nHelper Functions\nFunctions are sourced at the top so they stay out of the way while reading. Full helper code is shown here for reference.\n r_beta_bernoulli = function(n, alpha, beta) {\n  probs = rbeta(n, alpha, beta)\n  sample = rbinom(n, 1, probs)\n  return(sample)\n}\n\nr_beta_binom = function(n, size, alpha, beta) {\n  probs = rbeta(n, alpha, beta)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nr_markov_bernoulli = function(n, p0, drift_sd) {\n  probs = numeric(n)\n  probs[1] = p0\n  \n  for (i in 2:n) {\n    probs[i] = probs[i - 1] + rnorm(1, 0, drift_sd)\n    probs[i] = min(max(probs[i], 0), 1)\n  }\n  sample = rbinom(n, 1, probs)\n  return(sample)\n}\n\nr_markov_counts = function(n, size, p0, drift_sd) {\n  sample = numeric(n)\n  for (i in 1:n) {\n    sample[i] = sum(r_markov_bernoulli(size, p0[i], drift_sd[i]))\n  }\n  return(sample)\n}\n\npp_sample_distance = function(pp_sample1, pp_sample2, method = \"tvd\") {\n  \n  if (length(pp_sample1) != length(pp_sample2)) {\n    print(\"warning: sample sizes are different\")\n  }\n  \n  if (method == \"tvd\") {\n    max_val = max(c(pp_sample1, pp_sample2))\n    \n    counts1 = tabulate(pp_sample1, nbins = max_val)\n    counts2 = tabulate(pp_sample2, nbins = max_val)\n    \n    counts1 = counts1 / sum(counts1)\n    counts2 = counts2 / sum(counts2)\n    \n    return(0.5 * sum(abs(counts1 - counts2)))\n  } else if (method == \"binned_tvd\") {\n    combined = c(pp_sample1, pp_sample2)\n    breaks = hist(combined, plot = FALSE)$breaks\n  \n    counts1 = hist(pp_sample1, breaks = breaks, plot = FALSE)$counts\n    counts2 = hist(pp_sample2, breaks = breaks, plot = FALSE)$counts\n    \n    counts1 = counts1 / sum(counts1)\n    counts2 = counts2 / sum(counts2)\n    \n    return(0.5 * sum(abs(counts1 - counts2)))\n  } else {\n    print(\"available methods are 'tvd' and 'binned_tcd'\")\n  }\n}\n\nplot_pp_difference = function(real_sample, miss_sample, prior_or_post, real_model_name, miss_model_name) {\n  \n  plot_df = tibble(\n   D = real_sample,\n   M = miss_sample\n  ) |&gt; \n  pivot_longer(\n    cols = D:M,\n    names_to = \"source\",\n    values_to = \"values\"\n  )\n\n  ggplot(plot_df, aes(x = values, fill = source)) +\n    geom_histogram(aes(y = after_stat(density)), position = \"identity\", alpha = 0.5, bins = 30) +\n    labs(title = glue(\"{prior_or_post} Predictive Distribution Comparison\"), x = \"Count\", y = \"Density\") +\n    scale_fill_discrete(\n      labels = c(glue(\"Real Model ({real_model_name})\"), glue(\"Misspecified Model ({miss_model_name})\"))) + \n    theme_minimal()\n}\n\nsample_from_pp_D1 = function(n, size, mu1, sigma1, mu2, sigma2) {\n  alphas = rlnorm(n, mu1, sigma1)\n  betas = rlnorm(n, mu2, sigma2)\n  probs = rbeta(n, alphas, betas)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nsample_from_pp_D2 = function(n, size, alpha, beta, rate) {\n    p0 = rbeta(n, alpha, beta)\n    drift_sd = rexp(n, rate)\n    sample = r_markov_counts(n, size, p0, drift_sd)\n    return(sample)\n}\n\nr_markov_bernoulli_old = function(n, p_initial, p_after_1, p_after_0){\n  sample = numeric(n)\n  sample[1] = rbinom(1, 1, p_initial)\n  for (i in 2:n) {\n    if (sample[i - 1] == 1) {sample[i] = rbinom(1, 1, p_after_1)}\n    else {sample[i] = rbinom(1, 1, p_after_0)}\n  }\n  return(sample)\n}\n\nr_markov_counts_old = function(n, size, p_initial, p_after_1, p_after_0) {\n  sample = numeric(n)\n  for (i in 1:n) {\n    sample[i] = sum(r_markov_bernoulli_old(size, p_initial, p_after_1, p_after_0))\n  }\n  return(sample)\n} \n\n\nErrata\nWe tried using grid search over parameterized priors to find priors with equivalent prior predictive distributions, but it did not work well at all.\n\n# grid = expand.grid(\n#   mu1 = seq(1, 50, by=5),\n#   mu2 = seq(1, 50, by=5),\n#   sigma1 = seq(1, 50, by=5),\n#   sigma2 = seq(1, 50, by=5)\n# )\n\n# grid = grid |&gt; \n#   mutate(\n#     pp_sample = pmap(\n#       list(mu1, sigma1, mu2, sigma2),\n#       ~ sample_from_pp_D1(n = pp_sample_size,\n#                           size = n_obs,\n#                           mu1 = ..1, sigma1 = ..2,\n#                           mu2 = ..3, sigma2 = ..4))) |&gt; \n#   mutate(\n#     distances = map(pp_sample, ~ pp_sample_distance(.x, prior_pred_M))\n#   ) |&gt; \n#   arrange(distances)\n\nWe tried to use a different kind of markov model originally, but could not find priors which made equivalent prior predictive distributions for it.\n\n# (UNUSED because couldn't find equivalent predictive prior) function to sample Markov Bernoulli Distribution\nr_markov_bernoulli_old = function(n, p_initial, p_after_1, p_after_0){\n  sample = numeric(n)\n  sample[1] = rbinom(1,1,p_initial)\n  for (i in 2:n) {\n    if (sample[i-1]==1) {sample[i] = rbinom(1,1, p_after_1)}\n    else {sample[i] = rbinom(1,1, p_after_0)}\n  }\n  return(sample)\n}\n\n# (UNUSED because couldn't find equivalent predictive prior) function to sample counts from 'size'-number of markov bernoulli trials\nr_markov_counts_old = function(n, size, p_initial, p_after_1, p_after_0) {\n  sample = numeric(n)\n  for (i in 1:n){ # for each sample, do 'size' bernoulli trials, then sum\n    sample[i] = sum(r_markov_bernoulli(size, p_initial, p_after_1, p_after_0))\n  }\n  return(sample)\n}",
    "crumbs": [
      "Projects",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "projects/skincancer/index.html",
    "href": "projects/skincancer/index.html",
    "title": "Melanoma Risk Prediction, Kaggle ISIC 2024",
    "section": "",
    "text": "Link to Code"
  },
  {
    "objectID": "projects_archived/Stat365/index.html",
    "href": "projects_archived/Stat365/index.html",
    "title": "Narrative Visualization (Scratch)",
    "section": "",
    "text": "counts_male &lt;- age_gaps |&gt; \n  filter(character_1_gender == \"man\") |&gt; \n  count(age_difference)\ncounts_female &lt;- age_gaps |&gt; \n  filter(character_1_gender == \"woman\") |&gt; \n  count(age_difference)\n\nggplot() +\n  geom_col(data = counts_male, aes(x = age_difference, y = n), fill = \"steelblue\", width=1) +\n  geom_col(data = counts_female, aes(x = age_difference, y = -n), fill = \"pink\", width=1) +\n  geom_label(aes(x=30, y=40, label=\"Male\"), color=\"steelblue\")+\n  geom_label(aes(x=30, y=-20, label=\"Female\"), color=\"pink\")+\n  labs(title = \"Age Differences in Movie Couples\",\n       subtitle = \"By Gender of the Older Actor\",\n       x='', y='',\n       caption = \"Data sourced from hollywoodagegap.com.\") +\n  theme_minimal()\n\n\n\n\nFigure 1.2 shows the age differences in holywood movie couples. It’s clear that male older relationships are significantly more common than female older relationships, as well as having larger average age gaps.\n\n\n\n\n\nage_gaps &lt;- age_gaps |&gt;\n  mutate(maleFemDiff = case_when(\n    character_1_gender == \"woman\" ~ -1 * age_difference,\n    character_1_gender == \"man\" ~ age_difference\n  ))\n\nggplot() +\n  geom_histogram(data = age_gaps %&gt;% filter(maleFemDiff &gt;= 0), aes(x = maleFemDiff), fill = \"steelblue\", binwidth = 1) +\n  geom_histogram(data = age_gaps %&gt;% filter(maleFemDiff &lt; 0), aes(x = maleFemDiff), fill = \"pink\", binwidth = 1) +\n  geom_label(aes(x =20, y = 35, label = \"Male\"), color = \"steelblue\") +\n  geom_label(aes(x = -20, y = 35, label = \"Female\"), color = \"pink\")+\n    labs(\n    title = \"Age Differences in Heterosexual Movie Couples\",\n    subtitle = \"By age of Male character - Female character\",\n    y=\"Count\",\n    x=\"Male age - Female age\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# normality check:\nqqnorm(age_gaps$maleFemDiff)\nqqline(age_gaps$maleFemDiff)\n\n\n\n\n\n\n\n# t.test\nt.test(age_gaps$maleFemDiff, mu=0, alternative=\"greater\")\n\n\n    One Sample t-test\n\ndata:  age_gaps$maleFemDiff\nt = 27.434, df = 1154, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 7.946419      Inf\nsample estimates:\nmean of x \n  8.45368"
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "Stats",
    "section": "",
    "text": "Statistics and inference projects.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects_scratch/skincancer/index.html",
    "href": "projects_scratch/skincancer/index.html",
    "title": "Skin Cancer Detection under Extreme Class Impabalance (ISIC 2024)",
    "section": "",
    "text": "Overview\nThis group project was part of a Artificial Intelligence course and explored supervised learning approaches for skin cancer detection using the ISIC 2024 dataset, which consists of over 400,000 lesion images extracted from 3D total body photography along with structured metadata. The task was to classify lesions as benign or malignant in a setting characterized by extreme class imbalance (approximately 1000:1 benign to malignant). The goal was not leaderboard optimization, but to understand how standard machine learning methods behave under severe imbalance and limited signal.\n\n\nData and Approach\nWe evaluated both metadata-based and image-based models. Metadata features included patient age, lesion size, and body location, which were cleaned, normalized, and encoded prior to modeling. For structured data, we implemented logistic regression (in PyTorch), random forests, and XGBoost. For image data, we trained several convolutional architectures ranging from a simple CNN to deeper models, including a ResNet-18 variant. Models were evaluated using malignant-class precision and recall rather than overall accuracy.\n\n\nKey Challenges and Findings\nThe dominant challenge was extreme class imbalance, which caused many models to default to predicting the benign class. We experimented with loss weighting and weighted sampling strategies to mitigate this effect; weighted sampling produced more stable training behavior but did not yield consistently strong predictive performance. Across both metadata and image models, training loss often failed to exhibit a clear downward trend, suggesting limited learnable signal under the given constraints. Some configurations achieved high recall at the expense of very low precision, limiting practical usefulness.\n\n\nInterpretation\nThis project highlighted the limits of standard supervised learning pipelines in highly imbalanced medical imaging tasks without substantial prior structure. The results suggest that meaningful improvements would likely require stronger inductive bias (e.g., pretrained representations, asymmetric or focal loss functions, or multi-task learning) or additional curated data. The primary contribution of this work lies in diagnosing failure modes, appropriate metric selection, and understanding when increased model complexity does not compensate for data limitations.\n\n\nTools\nPython, PyTorch, scikit-learn, XGBoost, convolutional neural networks, class-imbalance handling techniques.\n\nView Repository\nView Presentation Slides"
  },
  {
    "objectID": "projects_scratch/Stat365/index.html",
    "href": "projects_scratch/Stat365/index.html",
    "title": "Narrative Visualization (Scratch)",
    "section": "",
    "text": "counts_male &lt;- age_gaps |&gt; \n  filter(character_1_gender == \"man\") |&gt; \n  count(age_difference)\ncounts_female &lt;- age_gaps |&gt; \n  filter(character_1_gender == \"woman\") |&gt; \n  count(age_difference)\n\nggplot() +\n  geom_col(data = counts_male, aes(x = age_difference, y = n), fill = \"steelblue\", width=1) +\n  geom_col(data = counts_female, aes(x = age_difference, y = -n), fill = \"pink\", width=1) +\n  geom_label(aes(x=30, y=40, label=\"Male\"), color=\"steelblue\")+\n  geom_label(aes(x=30, y=-20, label=\"Female\"), color=\"pink\")+\n  labs(title = \"Age Differences in Movie Couples\",\n       subtitle = \"By Gender of the Older Actor\",\n       x='', y='',\n       caption = \"Data sourced from hollywoodagegap.com.\") +\n  theme_minimal()\n\n\n\n\nFigure 1.2 shows the age differences in holywood movie couples. It’s clear that male older relationships are significantly more common than female older relationships, as well as having larger average age gaps.\n\n\n\n\n\nage_gaps &lt;- age_gaps |&gt;\n  mutate(maleFemDiff = case_when(\n    character_1_gender == \"woman\" ~ -1 * age_difference,\n    character_1_gender == \"man\" ~ age_difference\n  ))\n\nggplot() +\n  geom_histogram(data = age_gaps %&gt;% filter(maleFemDiff &gt;= 0), aes(x = maleFemDiff), fill = \"steelblue\", binwidth = 1) +\n  geom_histogram(data = age_gaps %&gt;% filter(maleFemDiff &lt; 0), aes(x = maleFemDiff), fill = \"pink\", binwidth = 1) +\n  geom_label(aes(x =20, y = 35, label = \"Male\"), color = \"steelblue\") +\n  geom_label(aes(x = -20, y = 35, label = \"Female\"), color = \"pink\")+\n    labs(\n    title = \"Age Differences in Heterosexual Movie Couples\",\n    subtitle = \"By age of Male character - Female character\",\n    y=\"Count\",\n    x=\"Male age - Female age\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# normality check:\nqqnorm(age_gaps$maleFemDiff)\nqqline(age_gaps$maleFemDiff)\n\n\n\n\n\n\n\n# t.test\nt.test(age_gaps$maleFemDiff, mu=0, alternative=\"greater\")\n\n\n    One Sample t-test\n\ndata:  age_gaps$maleFemDiff\nt = 27.434, df = 1154, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 7.946419      Inf\nsample estimates:\nmean of x \n  8.45368"
  },
  {
    "objectID": "projects_scratch/transformers/index.html",
    "href": "projects_scratch/transformers/index.html",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "Transformer Circuits Thread (transformer-circuits.pub)\nTESTING EDIT\n\n\nprivileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -\n\n\n\nresidual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear\n\n\n\n\n\nT(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)\n\n\n\n\n\n\n“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  },
  {
    "objectID": "projects_scratch/transformers/index.html#things-i-dont-get",
    "href": "projects_scratch/transformers/index.html#things-i-dont-get",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "privileged basis definition\n“activation”, in the context of activation vs parameter\nhow exactly is position embedded into a vector space (in the space that the key and query vectors are in) - and if it even does encode position (or like subsets of positions within context) - and can this position embedding be more generalized to other methods of storing tokens, instead of just a rly long string - hierarchical - graphical -"
  },
  {
    "objectID": "projects_scratch/transformers/index.html#notes",
    "href": "projects_scratch/transformers/index.html#notes",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "residual stream: linear combination of inputs and attention layer outputs - it’s entirely linear, and acts as a subspace - attention layers perform projections onto the subspace - due to linearity, at any 2 points, can make a matrix which represents the whole transformation up to that point - ? could this be used in actual transformers to reduce the amount of computation used? - has to be in parts which are linear"
  },
  {
    "objectID": "projects_scratch/transformers/index.html#basic-definitions",
    "href": "projects_scratch/transformers/index.html#basic-definitions",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "T(t) : Predicted Logits from input tokens (the Transformer Output) (context x vocab) matrix - “Activation, Privileged Basis”\nt : a vector of tokens (each of which is a one-hot encoded vector) in the context (context x vocab) matrix - “Activation, Privileged Basis”\n\\(x^n\\) : vector of embedding vectors in the residual stream (at layer n) (context x semantic dims) matrix - each vector is the updated semantic representation of a context token after n layers - “Activation, Not Privileged Basis”\n\\(W_E\\) : Embedding Weights, a transformation from one-hot input vectors to embedding vectors (semantic dims x vocab) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the type of tokens - (frequency?) - Parameter\n\\(W_P\\) : Positional Embedding Weights, a transformation from position in context to embedding vector (semantic dims x context) transformation matrix - input is a vector of tokens (the context), output is a vector of embedding vectors - only takes into account the positions - (time?) - Parameter\n\\(W_U\\) : Unembedding Weights, a transformation from embedding vectors to vocab vectors (logits) (vocab x semantic dims) transformation matrix - (is this the inverse of W_E?) - includes softmax? - Parameter\n\n\n\n\\(H_n\\) : Attention Heads, the set of attention heads at a layer n\n\\(h \\in H_n\\) : An attention head, a function from embedding vectors to embedding vectors\n\\(h(x)\\) : The output of attention head \\(h\\) given some embedding vectors \\(x\\) (context, semantic dims) matrix - a vector of semantic embedding vectors of each token of the context\n\\(\\bf{A}^h\\) : Attention pattern of head h, Not sure what this is, the matrix representing the linear transformation? a transformation from embedded context vectors to embedded context vectors? (context, context) transformation matrix\nFor each head h: this is how you go from input \\(x_i\\) to output \\(h(x)_i\\) Informally: embedding vector -(Value Matrix)-&gt; value vector -(Attention Pattern)-&gt; result vector -(Output Matrix)&gt; output embedding vector Formally: \\(W_V​x_i = v_i\\) , then \\(\\sum_j{ A_{i, j} v_j} = r_i\\) , then \\(W_Or_i=h(x)_i\\)"
  },
  {
    "objectID": "projects_scratch/transformers/index.html#notes-1",
    "href": "projects_scratch/transformers/index.html#notes-1",
    "title": "Transformer Notes (Draft)",
    "section": "",
    "text": "“A privileged basis occurs when some aspect of a model’s architecture encourages neural network features to align with basis dimensions”\nConfusion: A privileged basis is when vectors (aka directions) have specific meanings and can’t be rotated without changing those meanings? - but don’t embedding values have meanings assigned to specific directions? unless the meanings are all relative to other vectors - what are the embedding values if their directions don’t have specific meanings - or maybe the directions do have specific meanings, but these meanings don’t have to align to particular neurons\nExamples: “for example (a privileged basis occurs) because of a sparse activation function such as ReLU”\nVectors which do not have a privileged basis: “it doesn’t really make sense to look at the”neurons” (ie. basis dimensions) of activations like the residual stream, keys, queries or values, which don’t have a privileged basis.” - residual stream / embedding - Output of attention head \\(h\\) - Query, key, value and result vectors of attention head \\(h\\) - Output of MLP layer \\(m\\)\nVectors which do have a privileged basis: - Transformer logits - One-hot encoded tokens - Attention pattern of attention head \\(h\\) - Activations of MLP layer \\(m\\)\n\n\n\nwhen combined, is the transformation of the input to output\nvalue matrix is the input token, output is the output token - and when multiplied together, they make up the transformation\nSpeculative what if you find the nearest token (or token sequence) to this final output value vector, to get a description\n\n\n\nFrom a point in embedding space, how to efficiently find a sequence of tokens which approximates that value - maybe given a minimum variance or distance from target - can we even actually represent sequences of tokens as a single point in the embedding space?\nhow to represent an arbitrary sequence of tokens as a single point in an embedding space - what are the scaling difficulties of representing more and more complex stuff in a single embedding space? - relationship of necessary dim size to token sequence length? - like maybe the more tokens in a sequence you’d like to represent, the more embedding dimensions you need to represent the semantic information - maybe instead of using the same embedding dim for everything, use different ones for different “levels” of token sequence length - sort of representing complexity? abstraction?\nconstruct graph relations from - attention to get"
  },
  {
    "objectID": "projects/senior_consulting/index.html",
    "href": "projects/senior_consulting/index.html",
    "title": "Statistical Consulting Senior Projects",
    "section": "",
    "text": "Overview\nThese projects were completed as part of an undergraduate capstone in statistical consulting, involving real data and stakeholders.\n\n\nProjects\n\nIBaby Month One: Maternal Depression and Technology Use\nStudied survey data from new mothers to see whether parenting stress and technology use are linked to depression one month after birth. Used a logistic regression model and found that higher stress and greater distraction were strongly associated with increased depression risk, even after accounting for basic demographic factors.\n\n\nSustainability Literacy: Environmental and Educational Predictors of Knowledge\nStudied how environmental background and education relate to sustainability knowledge, while controlling for time spent in college. Applied multiple linear regression to survey and census data and found that college-level exposure mattered more than individual background characteristics.\n\n\nManual Task Performance Under Distraction and Biomechanical Constraints\nStudied how hand dominance, wrist movement, and mental distraction influence performance on a manual task. Used a mixed-effects design to account for repeated measurements and focused on interpreting results with appropriate attention to statistical power.",
    "crumbs": [
      "Projects",
      "Statistical Consulting"
    ]
  },
  {
    "objectID": "projects/senior_consulting/index.html#introduction",
    "href": "projects/senior_consulting/index.html#introduction",
    "title": "Black Hole Visualizer",
    "section": "",
    "text": "TODO write more better explanation, tex up derivation\nUsing C++, SFML, and GLSL\n\nmy first C++ and graphics project so please forgive strange and bad design choices\n\nThe simulation marches a grid of light rays (one per pixel) from the camera and numerically approximates the null geodesic equations for the Schwarzschild metric. The light rays curve and hit objects, which determine the color of the pixel.\n\nnumerical approximation is done via RK4 with adaptive step size\n\nThe metric equations I used are unitless, so the black hole being simulated is not of any fixed size, and has some arbitrary scale. The color of the ring is given by an arbitrarily chosen inner ring temperature of 5000 Kelvin, which makes for a pretty golden color.\n\nLink to Repository"
  },
  {
    "objectID": "projects/senior_consulting/index.html#progress-pictures",
    "href": "projects/senior_consulting/index.html#progress-pictures",
    "title": "Black Hole Visualizer",
    "section": "Progress pictures",
    "text": "Progress pictures\n \n\nAdded background stars\nMade disk color more realistic: calculate temperature -&gt; approximate black body radiation -&gt; color\nMore realistic disk sizing, note that the photon ring is now visible! I’m not sure what that very thin ring inside the photon ring is, and it might be a bug, but that’s for future me to figure out.\n\n\n - Added adaptive step sizing and other optimizations - Fixed object edge rendering for arbitrary step size - Added prettier colors and a grid to see how spacetime curves, and it’s very clearly showing gravitational lensing\n\n\n\n\nOlder Screenshot\n\n\n\nImplementing RK4 for the null geodesic equations in 3D.\nParallelized everything via fragment shader, which was quite difficult, as debugging information could only be encoded through pixel color. I’ve since learned that I should have used a compute shader and passed to a simple fragment shader, which would have been easier to debug and manage.\n\n\n\n\n\nOlder Screenshot\n\n\n\nDerived the null geodesic equations in 2D via Lagrangian, here I learned the basics of general relativity.\nImplemented RK4 for numerically solving said equations.\n\n\n\n\n\nOlder Screenshot\n\n\n\nBuilt basic euclidean raymarching engine.\nMore of learning how to program in C++.\n\n\n\n\n\nOlder Screenshot\n\n\n\nDrew a circle!\nLearned the very basics of C++, SFML, and GLSL.\n\n\n\nContext\nI started this project independently to learn C++, shader programming, and touch on general relativity. It took about a month and is so far the most difficult project I’ve mostly completed. I plan to develop more but am really tired of thinking about this project for now.\n\nI learned C++ with this free book.\nI was able to learn some general relativity with Sean Carrol’s Spacetime and Geometry, which can be acquired through allegedly legal means.\nI was inspired to start this project by this cool video. I did not look at any of his code, and used a different derivation for the geodesic equations. I was also more generally inspired by Sebastian Lague’s very cool youtube channel to try shader programming."
  },
  {
    "objectID": "projects/senior_consulting/index.html#future-work",
    "href": "projects/senior_consulting/index.html#future-work",
    "title": "Black Hole Visualizer",
    "section": "Future work:",
    "text": "Future work:\n\nthe center of the bh has trouble rendering bc parameterizing with phi, I’ll try to minimize this by adjusting the dynamic step size to account for starting angle\nif you move, the camera turns on its own, because I’ve yet to implement parallel transport of the camera directions, but this will be done\naccretion disk adjustments\n\nneeds better boundary detection\nits shape should be flared out a little for more realism\nadd doppler effect coloring"
  },
  {
    "objectID": "projects/senior_consulting/index.html#overview",
    "href": "projects/senior_consulting/index.html#overview",
    "title": "Statistical Consulting Senior Projects",
    "section": "",
    "text": "These projects were completed as part of an undergraduate capstone in statistical consulting, involving real data and stakeholders."
  },
  {
    "objectID": "projects/senior_consulting/index.html#data",
    "href": "projects/senior_consulting/index.html#data",
    "title": "Statistical Consulting Senior Projects",
    "section": "",
    "text": "The analysis used survey responses from Cal Poly students combined with environmental and educational variables derived from home ZIP-code census data. The response variable was a sustainability knowledge score. Predictors included air quality (PM2.5 concentration), greenspace coverage, fire frequency, ZIP-code area, and neighborhood educational attainment. Data collection involved a mix of random classroom sampling and voluntary responses, which limited population-level generalization."
  },
  {
    "objectID": "projects/senior_consulting/index.html#methodology",
    "href": "projects/senior_consulting/index.html#methodology",
    "title": "Statistical Consulting Senior Projects",
    "section": "",
    "text": "I used multiple linear regression to estimate adjusted associations between background factors and sustainability knowledge. Two models were fit: one focusing on environmental variables and one on educational background variables. In both models, year in college was included as a control variable to account for institutional exposure. Interaction terms were intentionally excluded to reduce overfitting and preserve interpretability."
  },
  {
    "objectID": "projects/senior_consulting/index.html#key-findings",
    "href": "projects/senior_consulting/index.html#key-findings",
    "title": "Statistical Consulting Senior Projects",
    "section": "",
    "text": "Across both models, year in college was the strongest and most consistent predictor of higher sustainability knowledge scores, indicating that institutional exposure plays a dominant role. In the environmental model, higher PM2.5 concentration and larger ZIP-code area were associated with modest decreases in SKS. In the educational model, higher neighborhood college-degree attainment was associated with higher SKS, even after adjusting for year in college."
  },
  {
    "objectID": "projects/senior_consulting/index.html#interpretation-and-limitations",
    "href": "projects/senior_consulting/index.html#interpretation-and-limitations",
    "title": "Statistical Consulting Senior Projects",
    "section": "",
    "text": "The results suggest that curriculum exposure may have a larger impact on sustainability knowledge than background factors alone, supporting curriculum-level interventions. Findings are associative rather than causal, and self-selection in survey responses and ZIP-code-level aggregation introduce uncertainty that should be considered when applying results beyond the sampled population."
  },
  {
    "objectID": "projects/senior_consulting/index.html#tools",
    "href": "projects/senior_consulting/index.html#tools",
    "title": "Statistical Consulting Senior Projects",
    "section": "",
    "text": "R (linear modeling, data visualization), census and environmental datasets.\nFull technical memo (PDF)"
  },
  {
    "objectID": "projects/blackholesim/index.html#current-and-progress-pictures",
    "href": "projects/blackholesim/index.html#current-and-progress-pictures",
    "title": "Black Hole Simulation",
    "section": "Current and Progress pictures",
    "text": "Current and Progress pictures\n \n\nAdded background stars\nMade disk color more realistic: calculate temperature -&gt; approximate black body radiation -&gt; color\nMore realistic disk sizing, note that the photon ring is now visible! I’m not sure what that very thin ring inside the photon ring is, and it might be a bug, but that’s for future me to figure out.\n\n\n - Added adaptive step sizing and other optimizations - Fixed object edge rendering for arbitrary step size - Added prettier colors and a grid to see how spacetime curves, and it’s very clearly showing gravitational lensing\n\n\n\n\nOlder Screenshot\n\n\n\nImplementing RK4 for the null geodesic equations in 3D.\nParallelized everything via fragment shader, which was quite difficult, as debugging information could only be encoded through pixel color. I’ve since learned that I should have used a compute shader and passed to a simple fragment shader, which would have been easier to debug and manage.\n\n\n\n\n\nOlder Screenshot\n\n\n\nDerived the null geodesic equations in 2D via Lagrangian, here I learned the basics of general relativity.\nImplemented RK4 for numerically solving said equations.\n\n\n\n\n\nOlder Screenshot\n\n\n\nBuilt basic euclidean raymarching engine.\nMore of learning how to program in C++.\n\n\n\n\n\nOlder Screenshot\n\n\n\nDrew a circle!\nLearned the very basics of C++, SFML, and GLSL.\n\n\n\nContext\nI started this project independently to learn C++, shader programming, and touch on general relativity. It took about a month and is so far the most difficult project I’ve mostly completed. I plan to develop more but am really tired of thinking about this project for now.\n\nI learned C++ with this free book.\nI was able to learn some general relativity with Sean Carrol’s Spacetime and Geometry, which can be acquired through allegedly legal means.\nI was inspired to start this project by this cool video. I did not look at any of his code, and used a different derivation for the geodesic equations. I was also more generally inspired by Sebastian Lague’s very cool youtube channel to try shader programming."
  },
  {
    "objectID": "projects/blackholesim/index.html#screenshots",
    "href": "projects/blackholesim/index.html#screenshots",
    "title": "Black Hole Simulation",
    "section": "Screenshots",
    "text": "Screenshots\n\nThe resolution is low, but these were run on a laptop from 2017 with integrated graphics.\n\n \n\nAdded background stars\nMade disk color more realistic: calculate temperature -&gt; approximate black body radiation -&gt; color\nMore realistic disk sizing, note that the photon ring is now visible! I’m not sure what that very thin ring inside the photon ring is, and it might be a bug, but that’s for future me to figure out.\n\n\n - Added adaptive step sizing and other optimizations - Fixed object edge rendering for arbitrary step size - Added prettier colors and a grid to see how spacetime curves, and it’s very clearly showing gravitational lensing\n\n\n\n\nOlder Screenshot\n\n\n\nImplementing RK4 for the null geodesic equations in 3D.\nParallelized everything via fragment shader, which was quite difficult, as debugging information could only be encoded through pixel color. I’ve since learned that I should have used a compute shader and passed to a simple fragment shader, which would have been easier to debug and manage.\n\n\n\n\n\nOlder Screenshot\n\n\n\nDerived the null geodesic equations in 2D via Lagrangian, here I learned the basics of general relativity.\nImplemented RK4 for numerically solving said equations.\n\n\n\n\n\nOlder Screenshot\n\n\n\nBuilt basic euclidean raymarching engine.\nMore of learning how to program in C++.\n\n\n\n\n\nOlder Screenshot\n\n\n\nDrew a circle!\nLearned the very basics of C++, SFML, and GLSL.",
    "crumbs": [
      "Projects",
      "Black Hole Simulation"
    ]
  },
  {
    "objectID": "projects/bayesian/index.html#example-1.-real-data-is-hierarchical-but-assumed-to-be-flat",
    "href": "projects/bayesian/index.html#example-1.-real-data-is-hierarchical-but-assumed-to-be-flat",
    "title": "Analysis of Bayesian Model Misspecification",
    "section": "Example 1. Real data is hierarchical, but assumed to be flat",
    "text": "Example 1. Real data is hierarchical, but assumed to be flat\nOur data \\(y\\) follows the following hierarchical distribution, which we’ll denote \\(D_1\\).\n\\[\n\\begin{align}\nY &\\sim Binom(100, p) \\\\\np &\\sim Beta(3, 4)\n\\end{align}\n\\]\n\nn_obs = 100\ny = r_beta_bernoulli(n_obs, alpha = 3, 4)\ny_obs = sum(y)\ny_obs\n\n[1] 42\n\n\nNow, we will misspecify the model.\nWe now assume the data comes from a binomial distribution, and assign a beta(5,3) prior on the proportion, where Y is the sum of the data, We’ll call this model \\(M\\) (for “Misspecified”). \\[\n\\begin{aligned}\nY &\\sim \\text{Binom}(n = 100, p) \\\\\n(Prior)\\quad p &\\sim \\text{Beta}(5, 3)\n\\end{aligned}\n\\]\nPrior Predictive distribution:\n\npp_sample_size = 10000\n\n# the predictive distribution follows a beta-binomial\nprior_pred_M = r_beta_binom(pp_sample_size, n_obs, 5, 3)\n\nNow to find an equivalent prior for the real model\nThe real model (\\(D_1\\)): \\[\n\\begin{aligned}\nY &\\sim \\text{Binom}(n=100, p) \\\\\np &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n(Prior)\\quad \\alpha &\\sim \\log \\mathcal{N}(\\mu_1, \\sigma_1) \\\\\n(Prior)\\quad \\beta &\\sim \\log \\mathcal{N}(\\mu_2, \\sigma_2)\n\\end{aligned}\n\\]\nA function to sample from the prior predictive distribution:\n\n# function for sampling from prior predictive distrib of D2 model\nsample_from_pp_D1 = function(n, size, mu1, sigma1, mu2, sigma2) {\n  alphas = rlnorm(n, mu1, sigma1)\n  betas = rlnorm(n, mu2, sigma2)\n  probs = rbeta(n, alphas, betas)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nWe will use lognormal \\(\\log \\mathcal{N}(\\log 5,\\ 0.01)\\) and \\(\\log \\mathcal{N}(\\log 3,\\ 0.01)\\) as the priors for \\(\\alpha\\) and \\(\\beta\\) in model \\(D_1\\), chosen to create a very similar prior predictive distribution to the prior for \\(M\\).\nThese are the prior predictive distributions compared:\n\nprior_pred_D1 = sample_from_pp_D1(n = pp_sample_size,\n                                 size = n_obs,\n                                 mu1 = log(5), sigma1 = 0.01,\n                                 mu2 = log(3), sigma2 = 0.01)\n\ntvd_prior = pp_sample_distance(prior_pred_D1, prior_pred_M)\ntvd_prior\n\n[1] 0.0447\n\nplot_pp_difference(real_sample = prior_pred_D1, real_model_name = \"D1\",\n                   miss_sample = prior_pred_M, miss_model_name = \"M\",\n                   prior_or_post = \"Prior\")\n\n\n\n\n\n\n\n\nAs you can see, the predictive distributions for the two models are extremely similar, which suggests the two priors are effectively equivalent.\nThis implies that any differences observed in the posterior distributions after updating on data are likely attributable to differences in the models themselves, rather than the priors.\nNow let’s find the posteriors of each and see how they vary.\n\nUpdating \\(D_1\\)\nFinding posterior via naive simulation\n\n# We sample from the prior a lot, predict with the param values from the prior,\n# and keep if the prediction is equal to the observed data\n\ndf = data.frame(alpha = numeric(), beta = numeric())\n\ni = 1\nwhile (i &lt;= pp_sample_size) {\n  alpha = rlnorm(1, log(5), 0.01)\n  beta = rlnorm(1, log(3), 0.01)\n  \n  y_pred = r_beta_binom(1, n_obs, alpha, beta)\n  \n  if (y_pred == y_obs) {\n    df = rbind(df, data.frame(alpha=alpha, beta=beta))\n    i = i+1\n  }\n}\n\ndf &lt;- mutate(df, post_pred = r_beta_binom(n(), n_obs, alpha, beta))\n\n\n\nUpdating \\(M\\)\nThis is a straightforward application of the Beta-Binomial Prior.\n\\[\nY \\sim Binom(100, p), \\quad p \\sim Beta(5, 3)\n\\]\nso \\[\np \\mid Y = 42 \\sim Beta\\!\\big(5+42,\\ 3+58\\big)\n\\]\n\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n\n\n\nComparing analysis for \\(M\\) vs equivalent analysis for \\(D_1\\)\n\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M)\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D1\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n\n\n\n\n\n\n\n\nThe posterior predictive distributions are quite different, with a Total Variational Distance of 0.648 (a 14.4966443 time increase from the prior TVD), so the model misspecification in this case seems to have a significant impact on results."
  },
  {
    "objectID": "projects/bayesian/index.html#example-2.-real-data-is-dependent-but-assumed-to-be-independent",
    "href": "projects/bayesian/index.html#example-2.-real-data-is-dependent-but-assumed-to-be-independent",
    "title": "Analysis of Bayesian Model Misspecification",
    "section": "Example 2. Real data is dependent, but assumed to be independent",
    "text": "Example 2. Real data is dependent, but assumed to be independent\nNow, we explore what happens if our sample comes from a dependent model. Specifically, we generate a sample from summing a Markov Chain of \\(n\\) drifting Bernoulli trials, denoted as model \\(D_2\\).\n\\[\nY \\sim MC(n=100, \\ p_0=0.4, \\ \\delta=0.1)\n\\] (MC for Markov Counts, details for sampling are in the helper functions section)\nWhere for \\(n\\) trials, \\(p_0\\) is the initial probability of success, and \\(\\delta\\) is the standard deviation of gaussian drift added to the success probability after each trial.\n\nn_ops = 100\ny = r_markov_bernoulli(n_ops, p0=0.4, drift_sd=0.1)\ny_obs = sum(y)\ny_obs\n\n[1] 34\n\n\nAgain, we will misspecify the model by assuming the data comes from a binomial distribution. We assign the same \\(\\text{Beta}(5, 3)\\) prior on \\(p\\). Again, we call this model \\(M\\). \\[\n\\begin{aligned}\nY &\\sim \\text{Binom}(n=100, p) \\\\\n(Prior)\\quad p &\\sim \\text{Beta}(5, 3)\n\\end{aligned}\n\\]\nLet’s then find an equivalent prior for model \\(D_2\\). \\[\n\\begin{aligned}\nY &\\sim MC(n=100, p_0, \\delta) \\\\\n(Prior)\\quad p_0 &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n(Prior)\\quad \\delta &\\sim \\text{Exp}(\\lambda)\n\\end{aligned}\n\\] We’ll need a function to sample a prior predictive distribution for \\(D_2\\).\n\nsample_from_pp_D2 = function(n, size, alpha, beta, rate) {\n    p0 = rbeta(n, alpha, beta)\n    drift_sd = rexp(n, rate)\n    sample = r_markov_counts(n, size, p0, drift_sd)\n    return(sample)\n}\n\nLet’s use this prior distribution: \\[\n\\begin{aligned}\nY &\\sim MC(n=100, \\ p_0, \\ \\delta) \\\\\n(Prior)\\quad p_0 &\\sim \\text{Beta}(5, 3) \\\\\n(Prior)\\quad \\delta &\\sim \\text{Exp}(100)\n\\end{aligned}\n\\]\nAnd compare the prior predictive distributions:\n\nprior_pred_D2 = sample_from_pp_D2(n=pp_sample_size,\n                                  size = n_obs,\n                                  alpha=5, beta=3, rate=100)\n\ntvd_prior = pp_sample_distance(prior_pred_D2, prior_pred_M)\ntvd_prior\n\n[1] 0.0601\n\nplot_pp_difference(real_sample = prior_pred_D2, real_model_name=\"D2\",\n                   miss_sample = prior_pred_M, miss_model_name=\"M\",\n                   prior_or_post = \"Prior\")\n\n\n\n\n\n\n\n\nAs shown, the priors are effectively equivalent.\nLet’s proceed with updating both models to see what kind of discrepancy there is.\n\nUpdating \\(D_2\\)\nWe use naive simulation again to create a posterior sample for D2. (This one is much slower, so we’re also accepting if the predicted is within 1 of the observed, which makes the posterior a bit less accurate.)\n\ndf = data.frame(p0 = numeric(), drift_sd = numeric())\n\ni = 1\nwhile (i &lt;= pp_sample_size) {\n  p0 = rbeta(1, 5, 3)\n  drift_sd = rexp(1, 100)\n  \n  y_pred = r_markov_counts(1, n_obs, p0, drift_sd)\n  \n  if (abs(y_pred - y_obs) &lt;= 1) {\n    df = rbind(df, data.frame(p0=p0, drift_sd=drift_sd))\n    i = i+1\n  }\n}\n\ndf &lt;- mutate(df, post_pred = r_markov_counts(n(), n_obs, p0, drift_sd))\n\n\n\nUpdating \\(M\\)\nAgain, a straightforward updating of the Beta-Binomial Prior.\n\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n\n\n\nComparing Posterior Predictive Distributions:\n\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M, method = 'tvd')\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D2\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n\n\n\n\n\n\n\n\nThere is a Total Variational Distance of 0.214 (a 3.5607321 time increase from the prior TVD), and we can see from the plot, the posterior predictive distributions are visibly different."
  },
  {
    "objectID": "projects/bayesian/index.html#example-1.-real-data-is-hierarchical-but-assumed-to-be-flat.",
    "href": "projects/bayesian/index.html#example-1.-real-data-is-hierarchical-but-assumed-to-be-flat.",
    "title": "Analysis of Bayesian Model Misspecification",
    "section": "Example 1. Real data is hierarchical, but assumed to be flat.",
    "text": "Example 1. Real data is hierarchical, but assumed to be flat.\nOur data \\(y\\) follows the following hierarchical distribution, which we’ll denote \\(D_1\\).\n\\[\n\\begin{align}\nY &\\sim Binom(100, p) \\\\\np &\\sim Beta(3, 4)\n\\end{align}\n\\]\n\nn_obs = 100\ny = r_beta_bernoulli(n_obs, alpha = 3, 4)\ny_obs = sum(y)\ny_obs\n\n[1] 42\n\n\nNow, we will misspecify the model.\nWe now assume the data comes from a binomial distribution, and assign a beta(5,3) prior on the proportion, where Y is the sum of the data, We’ll call this model \\(M\\) (for “Misspecified”). \\[\n\\begin{aligned}\nY &\\sim \\text{Binom}(n = 100, p) \\\\\n(Prior)\\quad p &\\sim \\text{Beta}(5, 3)\n\\end{aligned}\n\\]\nPrior Predictive distribution:\n\npp_sample_size = 10000\n\n# the predictive distribution follows a beta-binomial\nprior_pred_M = r_beta_binom(pp_sample_size, n_obs, 5, 3)\n\nNow to find an equivalent prior for the real model\nThe real model (\\(D_1\\)): \\[\n\\begin{aligned}\nY &\\sim \\text{Binom}(n=100, p) \\\\\np &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n(Prior)\\quad \\alpha &\\sim \\log \\mathcal{N}(\\mu_1, \\sigma_1) \\\\\n(Prior)\\quad \\beta &\\sim \\log \\mathcal{N}(\\mu_2, \\sigma_2)\n\\end{aligned}\n\\]\nA function to sample from the prior predictive distribution:\n\n# function for sampling from prior predictive distrib of D2 model\nsample_from_pp_D1 = function(n, size, mu1, sigma1, mu2, sigma2) {\n  alphas = rlnorm(n, mu1, sigma1)\n  betas = rlnorm(n, mu2, sigma2)\n  probs = rbeta(n, alphas, betas)\n  sample = rbinom(n, size, probs)\n  return(sample)\n}\n\nWe will use lognormal \\(\\log \\mathcal{N}(\\log 5,\\ 0.01)\\) and \\(\\log \\mathcal{N}(\\log 3,\\ 0.01)\\) as the priors for \\(\\alpha\\) and \\(\\beta\\) in model \\(D_1\\), chosen to create a very similar prior predictive distribution to the prior for \\(M\\).\nThese are the prior predictive distributions compared:\n\nprior_pred_D1 = sample_from_pp_D1(n = pp_sample_size,\n                                 size = n_obs,\n                                 mu1 = log(5), sigma1 = 0.01,\n                                 mu2 = log(3), sigma2 = 0.01)\n\ntvd_prior = pp_sample_distance(prior_pred_D1, prior_pred_M)\ntvd_prior\n\n[1] 0.0447\n\nplot_pp_difference(real_sample = prior_pred_D1, real_model_name = \"D1\",\n                   miss_sample = prior_pred_M, miss_model_name = \"M\",\n                   prior_or_post = \"Prior\")\n\n\n\n\n\n\n\n\nAs you can see, the predictive distributions for the two models are extremely similar, which suggests the two priors are effectively equivalent.\nThis implies that any differences observed in the posterior distributions after updating on data are likely attributable to differences in the models themselves, rather than the priors.\nNow let’s find the posteriors of each and see how they vary.\n\nUpdating \\(D_1\\)\nFinding posterior via naive simulation\n\n# We sample from the prior a lot, predict with the param values from the prior,\n# and keep if the prediction is equal to the observed data\n\ndf = data.frame(alpha = numeric(), beta = numeric())\n\ni = 1\nwhile (i &lt;= pp_sample_size) {\n  alpha = rlnorm(1, log(5), 0.01)\n  beta = rlnorm(1, log(3), 0.01)\n  \n  y_pred = r_beta_binom(1, n_obs, alpha, beta)\n  \n  if (y_pred == y_obs) {\n    df = rbind(df, data.frame(alpha=alpha, beta=beta))\n    i = i+1\n  }\n}\n\ndf &lt;- mutate(df, post_pred = r_beta_binom(n(), n_obs, alpha, beta))\n\n\n\nUpdating \\(M\\)\nThis is a straightforward application of the Beta-Binomial Prior.\n\\[\nY \\sim Binom(100, p), \\quad p \\sim Beta(5, 3)\n\\]\nso \\[\np \\mid Y = 42 \\sim Beta\\!\\big(5+42,\\ 3+58\\big)\n\\]\n\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n\n\n\nComparing analysis for \\(M\\) vs equivalent analysis for \\(D_1\\)\n\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M)\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D1\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n\n\n\n\n\n\n\n\nThe posterior predictive distributions are quite different, with a Total Variational Distance of 0.648 (a 14.4966443 time increase from the prior TVD), so the model misspecification in this case seems to have a significant impact on results.",
    "crumbs": [
      "Projects",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "projects/bayesian/index.html#example-2.-real-data-is-dependent-but-assumed-to-be-independent.",
    "href": "projects/bayesian/index.html#example-2.-real-data-is-dependent-but-assumed-to-be-independent.",
    "title": "Analysis of Bayesian Model Misspecification",
    "section": "Example 2. Real data is dependent, but assumed to be independent.",
    "text": "Example 2. Real data is dependent, but assumed to be independent.\nNow, we explore what happens if our sample comes from a dependent model. Specifically, we generate a sample from summing a Markov Chain of \\(n\\) drifting Bernoulli trials, denoted as model \\(D_2\\).\n\\[\nY \\sim MC(n=100, \\ p_0=0.4, \\ \\delta=0.1)\n\\] (MC for Markov Counts, details for sampling are in the helper functions section)\nWhere for \\(n\\) trials, \\(p_0\\) is the initial probability of success, and \\(\\delta\\) is the standard deviation of gaussian drift added to the success probability after each trial.\n\nn_ops = 100\ny = r_markov_bernoulli(n_ops, p0=0.4, drift_sd=0.1)\ny_obs = sum(y)\ny_obs\n\n[1] 34\n\n\nAgain, we will misspecify the model by assuming the data comes from a binomial distribution. We assign the same \\(\\text{Beta}(5, 3)\\) prior on \\(p\\). Again, we call this model \\(M\\). \\[\n\\begin{aligned}\nY &\\sim \\text{Binom}(n=100, p) \\\\\n(Prior)\\quad p &\\sim \\text{Beta}(5, 3)\n\\end{aligned}\n\\]\nLet’s then find an equivalent prior for model \\(D_2\\). \\[\n\\begin{aligned}\nY &\\sim MC(n=100, p_0, \\delta) \\\\\n(Prior)\\quad p_0 &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n(Prior)\\quad \\delta &\\sim \\text{Exp}(\\lambda)\n\\end{aligned}\n\\] We’ll need a function to sample a prior predictive distribution for \\(D_2\\).\n\nsample_from_pp_D2 = function(n, size, alpha, beta, rate) {\n    p0 = rbeta(n, alpha, beta)\n    drift_sd = rexp(n, rate)\n    sample = r_markov_counts(n, size, p0, drift_sd)\n    return(sample)\n}\n\nLet’s use this prior distribution: \\[\n\\begin{aligned}\nY &\\sim MC(n=100, \\ p_0, \\ \\delta) \\\\\n(Prior)\\quad p_0 &\\sim \\text{Beta}(5, 3) \\\\\n(Prior)\\quad \\delta &\\sim \\text{Exp}(100)\n\\end{aligned}\n\\]\nAnd compare the prior predictive distributions:\n\nprior_pred_D2 = sample_from_pp_D2(n=pp_sample_size,\n                                  size = n_obs,\n                                  alpha=5, beta=3, rate=100)\n\ntvd_prior = pp_sample_distance(prior_pred_D2, prior_pred_M)\ntvd_prior\n\n[1] 0.0601\n\nplot_pp_difference(real_sample = prior_pred_D2, real_model_name=\"D2\",\n                   miss_sample = prior_pred_M, miss_model_name=\"M\",\n                   prior_or_post = \"Prior\")\n\n\n\n\n\n\n\n\nAs shown, the priors are effectively equivalent.\nLet’s proceed with updating both models to see what kind of discrepancy there is.\n\nUpdating \\(D_2\\)\nWe use naive simulation again to create a posterior sample for D2. (This one is much slower, so we’re also accepting if the predicted is within 1 of the observed, which makes the posterior a bit less accurate.)\n\ndf = data.frame(p0 = numeric(), drift_sd = numeric())\n\ni = 1\nwhile (i &lt;= pp_sample_size) {\n  p0 = rbeta(1, 5, 3)\n  drift_sd = rexp(1, 100)\n  \n  y_pred = r_markov_counts(1, n_obs, p0, drift_sd)\n  \n  if (abs(y_pred - y_obs) &lt;= 1) {\n    df = rbind(df, data.frame(p0=p0, drift_sd=drift_sd))\n    i = i+1\n  }\n}\n\ndf &lt;- mutate(df, post_pred = r_markov_counts(n(), n_obs, p0, drift_sd))\n\n\n\nUpdating \\(M\\)\nAgain, a straightforward updating of the Beta-Binomial Prior.\n\npost_predictive_M = r_beta_binom(pp_sample_size, n_obs, 5+y_obs, 3+n_obs-y_obs)\n\n\n\nComparing Posterior Predictive Distributions:\n\ntvd_post = pp_sample_distance(df$post_pred, post_predictive_M, method = 'tvd')\n\nplot_pp_difference(real_sample = df$post_pred, real_model_name = \"D2\",\n                   miss_sample = post_predictive_M, miss_model_name = \"M\",\n                   prior_or_post = \"Posterior\")\n\n\n\n\n\n\n\n\nThere is a Total Variational Distance of 0.214 (a 3.5607321 time increase from the prior TVD), and we can see from the plot, the posterior predictive distributions are visibly different.",
    "crumbs": [
      "Projects",
      "Bayesian Analysis"
    ]
  },
  {
    "objectID": "projects/senior_consulting/index.html#projects",
    "href": "projects/senior_consulting/index.html#projects",
    "title": "Statistical Consulting Senior Projects",
    "section": "Projects",
    "text": "Projects\n\nIBaby Month One: Maternal Depression and Technology Use\nAnalyzed postpartum survey data to evaluate whether parenting stress and technology-related distraction are associated with maternal depression at one month postpartum. Built an interpretable logistic regression model with careful variable selection, finding strong associations between higher stress, higher perceived distraction, and elevated depression risk, while accounting for demographic covariates.\n\n\nSustainability Literacy: Environmental and Educational Predictors of Knowledge\nEvaluated whether environmental and educational background variables were associated with sustainability knowledge scores, adjusting for time in college. Used multiple linear regression on survey and census-linked data; results suggested institutional exposure dominated background effects.\n\n\nManual Task Performance Under Distraction and Biomechanical Constraints\nAnalyzed how hand dominance, wrist motion direction, and cognitive distraction affect performance on a constrained manual task using a mixed-effects factorial design, with emphasis on repeated measures and power-aware interpretation."
  },
  {
    "objectID": "projects/senior_consulting/sustainability.html",
    "href": "projects/senior_consulting/sustainability.html",
    "title": "Sustainability Literacy: Environmental and Educational Predictors of Knowledge",
    "section": "",
    "text": "Overview\nThis project analyzed survey and census-linked data to understand factors associated with students’ sustainability literacy knowledge scores (SKS). The goal was to inform potential improvements to general education curriculum by identifying whether background environmental conditions or educational context meaningfully relate to sustainability knowledge, after accounting for time spent in college.\n\n\nData\nThe analysis used survey responses from Cal Poly students combined with environmental and educational variables derived from home ZIP-code census data. The response variable was a sustainability knowledge score. Predictors included air quality (PM2.5 concentration), greenspace coverage, fire frequency, ZIP-code area, and neighborhood educational attainment. Data collection involved a mix of random classroom sampling and voluntary responses, which limited population-level generalization.\n\n\nMethodology\nI used multiple linear regression to estimate adjusted associations between background factors and sustainability knowledge. Two models were fit: one focusing on environmental variables and one on educational background variables. In both models, year in college was included as a control variable to account for institutional exposure. Interaction terms were intentionally excluded to reduce overfitting and preserve interpretability.\n\n\nKey Findings\nAcross both models, year in college was the strongest and most consistent predictor of higher sustainability knowledge scores, indicating that institutional exposure plays a dominant role. In the environmental model, higher PM2.5 concentration and larger ZIP-code area were associated with modest decreases in SKS. In the educational model, higher neighborhood college-degree attainment was associated with higher SKS, even after adjusting for year in college.\n\n\nInterpretation and Limitations\nThe results suggest that curriculum exposure may have a larger impact on sustainability knowledge than background factors alone, supporting curriculum-level interventions. Findings are associative rather than causal, and self-selection in survey responses and ZIP-code-level aggregation introduce uncertainty that should be considered when applying results beyond the sampled population.\n\n\nTools\nR (linear modeling, data visualization), census and environmental datasets.\n\nView technical report",
    "crumbs": [
      "Projects",
      "Statistical Consulting",
      "Sustainability"
    ]
  },
  {
    "objectID": "projects/senior_consulting/sustainability.html#overview",
    "href": "projects/senior_consulting/sustainability.html#overview",
    "title": "Sustainability Literacy: Environmental and Educational Predictors of Knowledge",
    "section": "",
    "text": "This project analyzed survey and census-linked data to understand factors associated with students’ sustainability literacy knowledge scores (SKS). The goal was to inform potential improvements to general education curriculum by identifying whether background environmental conditions or educational context meaningfully relate to sustainability knowledge, after accounting for time spent in college."
  },
  {
    "objectID": "projects/senior_consulting/sustainability.html#data",
    "href": "projects/senior_consulting/sustainability.html#data",
    "title": "Sustainability Literacy: Environmental and Educational Predictors of Knowledge",
    "section": "Data",
    "text": "Data\nThe analysis used survey responses from Cal Poly students combined with environmental and educational variables derived from home ZIP-code census data. The response variable was a sustainability knowledge score. Predictors included air quality (PM2.5 concentration), greenspace coverage, fire frequency, ZIP-code area, and neighborhood educational attainment. Data collection involved a mix of random classroom sampling and voluntary responses, which limited population-level generalization."
  },
  {
    "objectID": "projects/senior_consulting/sustainability.html#methodology",
    "href": "projects/senior_consulting/sustainability.html#methodology",
    "title": "Sustainability Literacy: Environmental and Educational Predictors of Knowledge",
    "section": "Methodology",
    "text": "Methodology\nI used multiple linear regression to estimate adjusted associations between background factors and sustainability knowledge. Two models were fit: one focusing on environmental variables and one on educational background variables. In both models, year in college was included as a control variable to account for institutional exposure. Interaction terms were intentionally excluded to reduce overfitting and preserve interpretability."
  },
  {
    "objectID": "projects/senior_consulting/sustainability.html#key-findings",
    "href": "projects/senior_consulting/sustainability.html#key-findings",
    "title": "Sustainability Literacy: Environmental and Educational Predictors of Knowledge",
    "section": "Key Findings",
    "text": "Key Findings\nAcross both models, year in college was the strongest and most consistent predictor of higher sustainability knowledge scores, indicating that institutional exposure plays a dominant role. In the environmental model, higher PM2.5 concentration and larger ZIP-code area were associated with modest decreases in SKS. In the educational model, higher neighborhood college-degree attainment was associated with higher SKS, even after adjusting for year in college."
  },
  {
    "objectID": "projects/senior_consulting/sustainability.html#interpretation-and-limitations",
    "href": "projects/senior_consulting/sustainability.html#interpretation-and-limitations",
    "title": "Sustainability Literacy: Environmental and Educational Predictors of Knowledge",
    "section": "Interpretation and Limitations",
    "text": "Interpretation and Limitations\nThe results suggest that curriculum exposure may have a larger impact on sustainability knowledge than background factors alone, supporting curriculum-level interventions. Findings are associative rather than causal, and self-selection in survey responses and ZIP-code-level aggregation introduce uncertainty that should be considered when applying results beyond the sampled population."
  },
  {
    "objectID": "projects/senior_consulting/sustainability.html#tools",
    "href": "projects/senior_consulting/sustainability.html#tools",
    "title": "Sustainability Literacy: Environmental and Educational Predictors of Knowledge",
    "section": "Tools",
    "text": "Tools\nR (linear modeling, data visualization), census and environmental datasets."
  },
  {
    "objectID": "projects/senior_consulting/ibaby.html",
    "href": "projects/senior_consulting/ibaby.html",
    "title": "IBaby Month One: Maternal Depression, Stress, and Technology Use",
    "section": "",
    "text": "Overview\nThis project supported an ongoing research study examining associations between maternal depression, parenting stress, and technology-related distraction during the first month postpartum. Using survey-based measures and demographic covariates, we evaluated whether stress and perceived distraction were associated with depression risk at one month postpartum.\n\n\nData\nThe analysis used data from 65 mothers assessed one month after birth. Depression status was defined using a CES-D–based indicator (score ≥ 13). Predictors included summed parenting stress scores, perceived technology-related distraction, maternal age, household income, and marital status. To reduce dimensionality and improve stability given the sample size, summarized scale scores were used rather than individual survey items, and observations with substantial missing data were excluded.\n\n\nMethodology\nA logistic regression model was used to estimate associations between depression status and the predictors of interest. Backward stepwise selection was applied to balance interpretability with the limited sample size, and continuous predictors were standardized to facilitate comparison of effect sizes through odds ratios.\n\n\nKey Findings\nHigher parenting stress and higher perceived distraction were both associated with increased odds of depression at one month postpartum. Maternal age was negatively associated with depression risk, with older mothers showing lower odds of depression. Household income and marital status were also retained in the final model, indicating potential socioeconomic and contextual influences that warrant further investigation.\n\n\nModel Performance and Diagnostics\nThe final model showed good discrimination between depressed and non-depressed cases on the available data (ROC AUC ≈ 0.91). Diagnostic checks did not indicate major violations of logistic regression assumptions or problematic multicollinearity.\n\n\nInterpretation and Limitations\nThe results suggest that stress and technology-related distraction are meaningfully associated with early postpartum depression. Findings are observational and based on a modest sample, so effect estimates should be interpreted cautiously, particularly for demographic variables that may reflect sample imbalance or unmeasured confounding.\n\n\nTools\nSAS (logistic regression, model selection, diagnostics).\n\nView technical report",
    "crumbs": [
      "Projects",
      "Statistical Consulting",
      "IBaby"
    ]
  },
  {
    "objectID": "projects/senior_consulting/ibaby.html#overview",
    "href": "projects/senior_consulting/ibaby.html#overview",
    "title": "IBaby Month One: Predictors of Maternal Depression at One Month Postpartum",
    "section": "",
    "text": "This project supported an ongoing research study examining relationships between maternal depression, parenting stress, and technology-related distraction during the first month postpartum. Using survey-derived scales and demographic covariates, we built an interpretable predictive model to evaluate whether higher stress and higher technology-related distraction are associated with elevated depression risk at one month postpartum."
  },
  {
    "objectID": "projects/senior_consulting/ibaby.html#data",
    "href": "projects/senior_consulting/ibaby.html#data",
    "title": "IBaby Month One: Predictors of Maternal Depression at One Month Postpartum",
    "section": "Data",
    "text": "Data\nWe analyzed data from ~65 mothers assessed at one month postpartum. The primary outcome was a binary depression indicator derived from the CES-D scale (depressed if CES-D score ≥ 13). Key predictors included:\n\nParenting stress score (18-item Likert scale summed into a total stress score)\nPerceived technology-related distraction (standardized)\nMaternal age\nHousehold income (binary indicator for low income)\nMarital status (married to baby’s biological father vs. other)\n\nTo improve reliability and reduce overfitting risk given the sample size, we used summary scale scores rather than individual survey items. Variables with substantial missingness were removed, and observations with missing or withheld income were excluded."
  },
  {
    "objectID": "projects/senior_consulting/ibaby.html#methodology",
    "href": "projects/senior_consulting/ibaby.html#methodology",
    "title": "IBaby Month One: Predictors of Maternal Depression at One Month Postpartum",
    "section": "Methodology",
    "text": "Methodology\nBecause the outcome was binary, we used logistic regression to model the probability of depression as a function of stress, perceived distraction, and demographic covariates. To balance interpretability with the small sample size, we performed backward stepwise selection (retaining predictors significant at the 0.10 level during selection). Continuous predictors were standardized to enable effect-size comparison via odds ratios."
  },
  {
    "objectID": "projects/senior_consulting/ibaby.html#key-findings",
    "href": "projects/senior_consulting/ibaby.html#key-findings",
    "title": "IBaby Month One: Predictors of Maternal Depression at One Month Postpartum",
    "section": "Key Findings",
    "text": "Key Findings\nThe final model retained five predictors: stress score, perceived distraction, maternal age, household income, and marital status.\n\nHigher parenting stress was strongly associated with higher odds of depression (p = 0.002).\nHigher perceived distraction was strongly associated with higher odds of depression (p = 0.004).\nOlder maternal age was associated with lower odds of depression (p = 0.040).\nLow income and marital status were also associated with substantially higher odds of depression in this sample (p = 0.010 and p = 0.036, respectively), suggesting potential confounding, subgroup effects, or sampling-related artifacts that merit further investigation."
  },
  {
    "objectID": "projects/senior_consulting/ibaby.html#model-performance-and-diagnostics",
    "href": "projects/senior_consulting/ibaby.html#model-performance-and-diagnostics",
    "title": "IBaby Month One: Predictors of Maternal Depression at One Month Postpartum",
    "section": "Model Performance and Diagnostics",
    "text": "Model Performance and Diagnostics\nOn the available dataset, the model achieved ROC AUC ≈ 0.915 and accuracy ≈ 74%, indicating strong separation between depressed vs. non-depressed classifications. Standard diagnostic checks did not indicate major violations of key logistic regression assumptions (e.g., lack of strong multicollinearity; no clear evidence against linearity in the logit for continuous predictors)."
  },
  {
    "objectID": "projects/senior_consulting/ibaby.html#interpretation-and-limitations",
    "href": "projects/senior_consulting/ibaby.html#interpretation-and-limitations",
    "title": "IBaby Month One: Predictors of Maternal Depression at One Month Postpartum",
    "section": "Interpretation and Limitations",
    "text": "Interpretation and Limitations\nResults support the study hypothesis that higher postpartum stress and higher technology-related distraction are associated with elevated depression risk at one month postpartum. Because the data are observational and the sample size is modest, findings should be interpreted as associations, not causal effects. Some estimated effects (notably income and marital status) were large, which may reflect small-sample instability, imbalanced subgroups, or unmeasured confounding; these variables are best treated as signals for follow-up rather than definitive conclusions."
  },
  {
    "objectID": "projects/senior_consulting/ibaby.html#tools",
    "href": "projects/senior_consulting/ibaby.html#tools",
    "title": "IBaby Month One: Predictors of Maternal Depression at One Month Postpartum",
    "section": "Tools",
    "text": "Tools\nR (logistic regression, stepwise selection, ROC/AUC evaluation, diagnostics).\n\nView technical report"
  },
  {
    "objectID": "projects/senior_consulting/mobility.html",
    "href": "projects/senior_consulting/mobility.html",
    "title": "Manual Task Performance Under Distraction and Biomechanical Constraints",
    "section": "",
    "text": "Overview\nThis project examined how cognitive distraction, screw-turning direction, and hand dominance affect performance on a constrained manual task. Participants unscrewed bolts under controlled conditions designed to mimic restricted wrist posture, allowing us to evaluate biomechanical and cognitive factors influencing task completion time.\n\n\nData and Experimental Design\nData were collected from 57 Cal Poly students. Each participant completed four trials using all combinations of dominant vs. nondominant hand and flexion vs. extension screw direction. Distraction (mental math vs. normal background noise) was assigned between subjects. The outcome was time to fully unscrew a bolt (seconds), yielding 228 total observations. The study followed a mixed 2×2×2 factorial design with repeated measures on hand and direction.\n\n\nMethodology\nWe analyzed log-transformed completion times using a mixed-effects ANOVA model to better satisfy normality assumptions. Distraction, hand, and direction were treated as fixed effects, with participant modeled as a random effect to account for repeated measures. The initial model included all main effects and interactions; non-informative interaction terms were removed to improve interpretability and model fit, leaving the hand-by-direction interaction.\n\n\nKey Findings\nScrew direction had a large effect on performance: extension trials were substantially slower than flexion trials. Hand dominance also mattered, with slower performance using the nondominant hand. There was moderate evidence of an interaction between hand and direction, suggesting that the nondominant-hand penalty was less pronounced during extension than flexion. Distraction showed no measurable effect on task time, nor did it interact meaningfully with hand or direction.\n\n\nPower Analysis and Design Implications\nThe study was well powered to detect large biomechanical effects but underpowered for small effects such as distraction or subtle hand differences. A power analysis indicated that substantially more participants would be required to reliably detect small time differences. As a result, a key recommendation is to treat distraction as a within-subject factor in future studies, which would improve efficiency and sensitivity without increasing sample size.\n\n\nTools\nR (mixed-effects modeling, ANOVA, power analysis).\n\nView technical report",
    "crumbs": [
      "Projects",
      "Statistical Consulting",
      "Mobility"
    ]
  },
  {
    "objectID": "projects/senior_consulting/mobility.html#overview",
    "href": "projects/senior_consulting/mobility.html#overview",
    "title": "Manual Task Performance Under Distraction and Biomechanical Constraints",
    "section": "",
    "text": "This project examined how cognitive distraction, screw-turning direction, and hand dominance affect performance on a constrained manual task. Participants unscrewed bolts under controlled conditions designed to mimic restricted wrist posture, allowing us to evaluate biomechanical and cognitive factors influencing task completion time."
  },
  {
    "objectID": "projects/senior_consulting/mobility.html#data-and-experimental-design",
    "href": "projects/senior_consulting/mobility.html#data-and-experimental-design",
    "title": "Manual Task Performance Under Distraction and Biomechanical Constraints",
    "section": "Data and Experimental Design",
    "text": "Data and Experimental Design\nData were collected from 57 Cal Poly students. Each participant completed four trials using all combinations of dominant vs. nondominant hand and flexion vs. extension screw direction. Distraction (mental math vs. normal background noise) was assigned between subjects. The outcome was time to fully unscrew a bolt (seconds), yielding 228 total observations. The study followed a mixed 2×2×2 factorial design with repeated measures on hand and direction."
  },
  {
    "objectID": "projects/senior_consulting/mobility.html#methodology",
    "href": "projects/senior_consulting/mobility.html#methodology",
    "title": "Manual Task Performance Under Distraction and Biomechanical Constraints",
    "section": "Methodology",
    "text": "Methodology\nWe analyzed log-transformed completion times using a mixed-effects ANOVA model to better satisfy normality assumptions. Distraction, hand, and direction were treated as fixed effects, with participant modeled as a random effect to account for repeated measures. The initial model included all main effects and interactions; non-informative interaction terms were removed to improve interpretability and model fit, leaving the hand-by-direction interaction."
  },
  {
    "objectID": "projects/senior_consulting/mobility.html#key-findings",
    "href": "projects/senior_consulting/mobility.html#key-findings",
    "title": "Manual Task Performance Under Distraction and Biomechanical Constraints",
    "section": "Key Findings",
    "text": "Key Findings\nScrew direction had a large effect on performance: extension trials were substantially slower than flexion trials. Hand dominance also mattered, with slower performance using the nondominant hand. There was moderate evidence of an interaction between hand and direction, suggesting that the nondominant-hand penalty was less pronounced during extension than flexion. Distraction showed no measurable effect on task time, nor did it interact meaningfully with hand or direction."
  },
  {
    "objectID": "projects/senior_consulting/mobility.html#power-analysis-and-design-implications",
    "href": "projects/senior_consulting/mobility.html#power-analysis-and-design-implications",
    "title": "Manual Task Performance Under Distraction and Biomechanical Constraints",
    "section": "Power Analysis and Design Implications",
    "text": "Power Analysis and Design Implications\nThe study was well powered to detect large biomechanical effects but underpowered for small effects such as distraction or subtle hand differences. A power analysis indicated that substantially more participants would be required to reliably detect small time differences. As a result, a key recommendation is to treat distraction as a within-subject factor in future studies, which would improve efficiency and sensitivity without increasing sample size."
  },
  {
    "objectID": "projects/senior_consulting/mobility.html#tools",
    "href": "projects/senior_consulting/mobility.html#tools",
    "title": "Manual Task Performance Under Distraction and Biomechanical Constraints",
    "section": "Tools",
    "text": "Tools\nR (mixed-effects modeling, ANOVA, power analysis).\n\nView technical report"
  },
  {
    "objectID": "projects_scratch/skincancer/index.html#overview",
    "href": "projects_scratch/skincancer/index.html#overview",
    "title": "Skin Cancer Detection under Extreme Class Impabalance (ISIC 2024)",
    "section": "",
    "text": "This project explored supervised learning approaches for skin cancer detection using the ISIC 2024 dataset, which consists of over 400,000 lesion images extracted from 3D total body photography along with structured metadata. The task was to classify lesions as benign or malignant in a setting characterized by extreme class imbalance (approximately 1000:1 benign to malignant). The goal was not leaderboard optimization, but to understand how standard machine learning methods behave under severe imbalance and limited signal."
  },
  {
    "objectID": "projects_scratch/skincancer/index.html#data-and-approach",
    "href": "projects_scratch/skincancer/index.html#data-and-approach",
    "title": "Skin Cancer Detection under Extreme Class Impabalance (ISIC 2024)",
    "section": "Data and Approach",
    "text": "Data and Approach\nWe evaluated both metadata-based and image-based models. Metadata features included patient age, lesion size, and body location, which were cleaned, normalized, and encoded prior to modeling. For structured data, we implemented logistic regression (in PyTorch), random forests, and XGBoost. For image data, we trained several convolutional architectures ranging from a simple CNN to deeper models, including a ResNet-18 variant. Models were evaluated using malignant-class precision and recall rather than overall accuracy."
  },
  {
    "objectID": "projects_scratch/skincancer/index.html#key-challenges-and-findings",
    "href": "projects_scratch/skincancer/index.html#key-challenges-and-findings",
    "title": "Skin Cancer Detection under Extreme Class Impabalance (ISIC 2024)",
    "section": "Key Challenges and Findings",
    "text": "Key Challenges and Findings\nThe dominant challenge was extreme class imbalance, which caused many models to default to predicting the benign class. We experimented with loss weighting and weighted sampling strategies to mitigate this effect; weighted sampling produced more stable training behavior but did not yield consistently strong predictive performance. Across both metadata and image models, training loss often failed to exhibit a clear downward trend, suggesting limited learnable signal under the given constraints. Some configurations achieved high recall at the expense of very low precision, limiting practical usefulness."
  },
  {
    "objectID": "projects_scratch/skincancer/index.html#interpretation",
    "href": "projects_scratch/skincancer/index.html#interpretation",
    "title": "Skin Cancer Detection under Extreme Class Impabalance (ISIC 2024)",
    "section": "Interpretation",
    "text": "Interpretation\nThis project highlighted the limits of standard supervised learning pipelines in highly imbalanced medical imaging tasks without substantial prior structure. The results suggest that meaningful improvements would likely require stronger inductive bias (e.g., pretrained representations, asymmetric or focal loss functions, or multi-task learning) or additional curated data. The primary contribution of this work lies in diagnosing failure modes, appropriate metric selection, and understanding when increased model complexity does not compensate for data limitations."
  },
  {
    "objectID": "projects_scratch/skincancer/index.html#tools",
    "href": "projects_scratch/skincancer/index.html#tools",
    "title": "Skin Cancer Detection under Extreme Class Impabalance (ISIC 2024)",
    "section": "Tools",
    "text": "Tools\nPython, PyTorch, scikit-learn, XGBoost, convolutional neural networks, class-imbalance handling techniques.\nView Repository\nView Presentation Slides"
  },
  {
    "objectID": "projects/ml_experiments/index.html#projects",
    "href": "projects/ml_experiments/index.html#projects",
    "title": "Machine Learning Experiments",
    "section": "Projects",
    "text": "Projects\n\nTuned Lens Exploration\nExploratory mechanistic interpretability experiment investigating Tuned Lenses by decomposing the lens-aligned predictive space into output-relevant and weakly output-relevant subspaces and probing their effects via targeted ablations, highlighting both diagnostic successes and sensitivity to lens quality.\n\n\nSkin Cancer Detection\nGroup project from a university course exploring skin cancer classification under extreme class imbalance using the ISIC 2024 dataset, with emphasis on diagnosing model failure modes, metric choice, and limits of standard supervised learning approaches rather than leaderboard performance.",
    "crumbs": [
      "Projects",
      "ML Experiments"
    ]
  },
  {
    "objectID": "projects/ml_experiments/skincancer.html",
    "href": "projects/ml_experiments/skincancer.html",
    "title": "Skin Cancer Detection under Extreme Class Imbalance (ISIC 2024)",
    "section": "",
    "text": "Overview\nThis group project was part of an Artificial Intelligence course and focused on using supervised learning methods to detect skin cancer with the ISIC 2024 dataset. The dataset contains over 400,000 skin lesion images from 3D total body photography, along with structured patient and lesion metadata. The task was to classify lesions as benign or malignant in an extremely imbalanced setting, with roughly 1,000 benign cases for every malignant case. The goal was not to optimize leaderboard performance, but to study how common machine learning methods behave under severe class imbalance and weak signal.\n\n\nData and Approach\nWe trained models using both structured metadata and raw images. Metadata included variables such as patient age, lesion size, and body location, which were cleaned and encoded before modeling. For structured data, we used logistic regression (implemented in PyTorch), random forests, and XGBoost. For image data, we trained several convolutional neural networks, ranging from a simple CNN to a ResNet-18–based model. Model performance was evaluated using precision and recall for the malignant class rather than overall accuracy.\n\n\nKey Challenges and Findings\nThe main challenge was extreme class imbalance, which caused many models to overwhelmingly predict the benign class. We tested loss weighting and weighted sampling to address this issue; weighted sampling led to more stable training but did not consistently improve predictive performance. Across both metadata-based and image-based models, training loss often showed little improvement, suggesting that the available signal was limited. Some models achieved high recall by predicting many malignant cases, but this came at the cost of very low precision, reducing practical usefulness.\n\n\nInterpretation\nThis project demonstrated the limitations of standard supervised learning pipelines when applied to highly imbalanced medical imaging problems without strong prior structure. The results suggest that better performance would likely require stronger inductive bias, such as pretrained image representations, specialized loss functions, or multi-task learning, as well as higher-quality or more targeted data. The main contribution of this work was identifying failure modes, choosing appropriate evaluation metrics, and understanding when increasing model complexity does not overcome data constraints.\n\n\nTools\nPython, PyTorch, scikit-learn, XGBoost, convolutional neural networks, class imbalance techniques\n\nView Repository\nView Presentation Slides",
    "crumbs": [
      "Projects",
      "ML Experiments",
      "Skin Cancer"
    ]
  },
  {
    "objectID": "projects/ml_experiments/tunedlens.html",
    "href": "projects/ml_experiments/tunedlens.html",
    "title": "Exploring Tuned Lenses via Subspace Ablations",
    "section": "",
    "text": "Background\nTuned Lenses are a technique from mechanistic interpretability research that let you read off the model’s internal predictions at intermediate layers. Instead of only looking at the final output, a tuned lens trains a small transformation that maps hidden states directly into the vocabulary’s space, letting you see what the model would predict based on partial computation at each layer.\n\nMore broadly, mechanistic interpretability is a field of AI research that tries to reverse-engineer how neural networks compute their outputs in human-understandable terms, rather than treating a model as a black box.\n\n\n\nMotivation\nTuned Lenses provide a way to map intermediate model activations into a shared predictive space aligned with the final token distribution. This experiment explored whether that lens-aligned space admits a meaningful linear decomposition into directions that directly influence output logits versus directions with little direct effect, and whether such a distinction can be probed through targeted ablations.\n\n\nCore Idea\nUsing a singular value decomposition of the unembedding matrix, we partitioned the lens-aligned predictive space into two subspaces: directions associated with large singular values (hypothesized to have strong influence on logits) and directions associated with small singular values (hypothesized to be weakly output-relevant). We then ablated activations within each subspace and measured changes in model predictions using KL divergence.\n\n\nResults\nWith pretrained Tuned Lenses, ablations of high–singular-value directions consistently produced substantially larger KL divergence than ablations of low–singular-value directions, with the effect generally increasing in later layers. In contrast, lenses trained on a small, low-quality subset of text data failed to show this separation, suggesting that the experiment is highly sensitive to lens quality and training distribution.\n\n\nInterpretation and Limitations\nThe results support the view that the lens-aligned space contains linearly identifiable directions with direct influence on the output distribution. However, conclusions are limited by dependence on pretrained lenses, lack of out-of-distribution validation, and the exploratory nature of the analysis. The experiment is best interpreted as a diagnostic probe of Tuned Lens behavior rather than a definitive decomposition of model computation.\n\n\nStatus\nThis work is exploratory and incomplete. Planned extensions included propagating output-subspace importance backward through lens maps to identify circuit endpoints, but these were not completed within the available time and infrastructure constraints.\n\nView Repository\n\n\nReferences\nBelrose, N., Henighan, T., Turner, A. M., et al. (2023).\nThe Tuned Lens: Revealing Representations of LLMs without Intervention.\narXiv:2303.08112. https://arxiv.org/abs/2303.08112",
    "crumbs": [
      "Projects",
      "ML Experiments",
      "Tuned Lens"
    ]
  },
  {
    "objectID": "projects/ml_experiments/tunedlens.html#motivation",
    "href": "projects/ml_experiments/tunedlens.html#motivation",
    "title": "Exploring Tuned Lenses via Subspace Ablations",
    "section": "",
    "text": "Tuned Lenses provide a way to map intermediate model activations into a shared predictive space aligned with the final token distribution. This experiment explored whether that lens-aligned space admits a meaningful linear decomposition into directions that directly influence output logits versus directions with little direct effect, and whether such a distinction can be probed through targeted ablations."
  },
  {
    "objectID": "projects/ml_experiments/tunedlens.html#core-idea",
    "href": "projects/ml_experiments/tunedlens.html#core-idea",
    "title": "Exploring Tuned Lenses via Subspace Ablations",
    "section": "Core Idea",
    "text": "Core Idea\nUsing a singular value decomposition of the unembedding matrix, we partitioned the lens-aligned predictive space into two subspaces: directions associated with large singular values (hypothesized to have strong influence on logits) and directions associated with small singular values (hypothesized to be weakly output-relevant). We then ablated activations within each subspace and measured changes in model predictions using KL divergence."
  },
  {
    "objectID": "projects/ml_experiments/tunedlens.html#results",
    "href": "projects/ml_experiments/tunedlens.html#results",
    "title": "Exploring Tuned Lenses via Subspace Ablations",
    "section": "Results",
    "text": "Results\nWith pretrained Tuned Lenses, ablations of high–singular-value directions consistently produced substantially larger KL divergence than ablations of low–singular-value directions, with the effect generally increasing in later layers. In contrast, lenses trained on a small, low-quality subset of text data failed to show this separation, suggesting that the experiment is highly sensitive to lens quality and training distribution."
  },
  {
    "objectID": "projects/ml_experiments/tunedlens.html#interpretation-and-limitations",
    "href": "projects/ml_experiments/tunedlens.html#interpretation-and-limitations",
    "title": "Exploring Tuned Lenses via Subspace Ablations",
    "section": "Interpretation and Limitations",
    "text": "Interpretation and Limitations\nThe results support the view that the lens-aligned space contains linearly identifiable directions with direct influence on the output distribution. However, conclusions are limited by dependence on pretrained lenses, lack of out-of-distribution validation, and the exploratory nature of the analysis. The experiment is best interpreted as a diagnostic probe of Tuned Lens behavior rather than a definitive decomposition of model computation."
  },
  {
    "objectID": "projects/ml_experiments/tunedlens.html#status",
    "href": "projects/ml_experiments/tunedlens.html#status",
    "title": "Exploring Tuned Lenses via Subspace Ablations",
    "section": "Status",
    "text": "Status\nThis work is exploratory and incomplete. Planned extensions included propagating output-subspace importance backward through lens maps to identify circuit endpoints, but these were not completed within the available time and infrastructure constraints.\n\nView Repository\n\nReferences\nBelrose, N., Henighan, T., Turner, A. M., et al. (2023).\nThe Tuned Lens: Revealing Representations of LLMs without Intervention.\narXiv:2303.08112. https://arxiv.org/abs/2303.08112"
  },
  {
    "objectID": "projects/ml_experiments/index.html",
    "href": "projects/ml_experiments/index.html",
    "title": "Machine Learning Experiments",
    "section": "",
    "text": "Experiments\n\nSkin Cancer Detection\nGroup project from a university course exploring skin cancer classification under extreme class imbalance using the ISIC 2024 dataset, with emphasis on diagnosing model failure modes, metric choice, and limits of standard supervised learning approaches rather than leaderboard performance.\n\n\nTuned Lens Exploration\nExploratory transformer interpretability experiment investigating Tuned Lenses by decomposing the lens-aligned predictive space into output-relevant and weakly output-relevant subspaces and probing their effects via targeted ablations.",
    "crumbs": [
      "Projects",
      "ML Experiments"
    ]
  },
  {
    "objectID": "projects/visualizations/index.html",
    "href": "projects/visualizations/index.html",
    "title": "Data Visualization Gallery",
    "section": "",
    "text": "Code\neverest_plot |&gt; \n  ggplot(aes(x = year, y = duration,\n             color = as.factor(success_indicator),\n             shape = as.factor(success_indicator))) +\n    geom_point(alpha=0.25) +\n    scale_color_manual(values = c(\"0\" = \"deepskyblue4\", \"1\" = \"darkorange3\"),\n                       labels = c(\"Unsuccessful\", \"Successful\")) +\n    scale_shape_manual(values = c(16, 17),\n                       labels = c(\"Unsuccessful\", \"Successful\")) +\n    labs(color = \"\", shape=\"\",\n         title = \"Duration and Success of Everest Expeditions Over Time\",\n         subtitle = \"Duration (days)\",\n         x = \"Year\",\n         y = \"\",\n         caption = \"Note: Success defined as reaching the summit.\") +\n    scale_x_continuous(breaks = seq(min(everest$year)-1, max(everest$year), by = 10))+\n    theme_minimal()\n\n\n\n\n\neverest-vis\n\n\nDot plot showing Mount Everest expedition lengths by year, with each point representing one expedition. Successful and unsuccessful climbs are shown separately, revealing that expedition durations have become more consistent over time, with sharp drops driven by external disasters. In 2014, an ice avalanche in the Khumbu Icefall killed several Sherpas and led to the cancellation of much of the climbing season. In 2015, a large earthquake in Nepal triggered avalanches that struck Everest Base Camp, forcing most expeditions to end early.\n\n\n\nCode\ncounts_male &lt;- age_gaps |&gt;\n  filter(character_1_gender == \"man\") |&gt;\n  count(age_difference)\n\ncounts_female &lt;- age_gaps |&gt;\n  filter(character_1_gender == \"woman\") |&gt;\n  count(age_difference)\n\nggplot() +\n  geom_col(\n    data = counts_male,\n    aes(x = age_difference, y = n),\n    fill = \"steelblue\"\n  ) +\n  geom_col(\n    data = counts_female,\n    aes(x = age_difference, y = -n),\n    fill = \"pink\"\n  ) +\n  geom_label(\n    aes(x = 30, y = 40, label = \"Male\")\n  ) +\n  geom_label(\n    aes(x = 30, y = -20, label = \"Female\")\n  ) +\n  labs(\n    title = \"Age Differences in Movie Couples\",\n    subtitle = \"By Gender of the Older Actor\",\n    x = \"Age Difference (years)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nage-gap-mirrored\n\n\nMirrored histogram comparing age differences in movie couples based on whether the older actor is male or female. Relationships with older men tend to have larger and more varied age gaps, while relationships with older women are more tightly clustered around smaller differences.\n\n\n\n\nage-gap-hist\n\n\nHistogram showing age differences in heterosexual movie couples, calculated as male age minus female age. Most values are positive, indicating that male actors are more often older than their female partners and by a wider range of ages.\n\n\n\nCode\nanimated_plot &lt;- ggplot(murder_happiness,\n                        aes(x = log(murder_rate_per_100k),\n                            y = happiness_score)) +\n  geom_point(color = \"steelblue\") +\n  geom_smooth(method = \"lm\", color = \"black\") +\n  labs(title = \"Relationship Between Murder Rate and Happiness Score (2005-2019)\",\n       subtitle = \"Happiness Score (0-100)\",\n       x = \"Log-Scaled Murder Rate (per 100k)\",\n       y = \"\",\n       caption = \"Year: {frame_time}\") +\n  transition_time(as.integer(year)) +\n  enter_fade() +\n  exit_fade() +\n  theme_bw() +\n  theme(plot.caption = element_text(size = 11))\n\nanimate(animated_plot, renderer = gifski_renderer())\n\n\n\n\n\nmurder-happiness-1\n\n\nScatter plot showing the relationship between murder rates and happiness scores across countries, with a fitted trend line that changes over time. Overall, higher murder rates tend to be associated with lower happiness, though the strength of this relationship varies by year.\n\n\n\nCode\nset.seed(42)\n\npredictions &lt;- predict(linear_model, country_murder_happiness)\nresidual_se &lt;- sigma(linear_model)\nsimulated_y &lt;- predictions + rnorm(n = length(predictions), mean = 0, sd = sigma(linear_model))\n\nobserved &lt;- ggplot(country_murder_happiness, \n             aes(x = log(avg_murder_rate), \n                 y = avg_happiness_score)\n             ) +\n  geom_point(color = \"steelblue\") +\n  labs(title = \"Observed Data\",\n       subtitle = \"Observed Happiness Score (0-100)\",\n       x = \"Log-Scaled Average Murder Rate (per 100k)\", \n       y = \"\") +\n  theme_bw()\n\n# Plot Simulated Data\npredicted &lt;- ggplot(country_murder_happiness, \n             aes(x = log(avg_murder_rate), \n                 y = simulated_y)\n             ) +\n  geom_point(color = \"orange3\") +\n  labs(title = \"Simulated Data\",\n       subtitle = \"Simulated Happiness Score (0-100)\",\n       x = \"Log-Scaled Average Murder Rate (per 100k)\", \n       y = \"\") +\n  theme_bw()\n\nobserved + predicted\n\n\n\n\n\nmurder-happiness-2\n\n\nSide-by-side scatter plots comparing the real data to data simulated from a linear regression model. The plots show that the model captures the general trend but produces more tightly clustered points than those seen in the observed data.\n\n\n\nCode\nggplot(data = penguins,\n       aes(x = bill_length_mm**2, y = log(bill_depth_mm),\n           shape = species, color = island)) +\n  geom_point(size = 0.1, alpha = 0.5) +\n  labs(title = \"PENGUIN\",\n       x = \"length (squared)\",\n       y = \"depth (log)\") +\n  annotate(\"rect\",\n           xmin = 500, xmax = 3240,\n           ymin = -0.7, ymax = 2.2,\n           fill = \"yellow\", alpha = 0.5) +\n  annotate(\"text\",\n           x = 1850, y = 1.3,\n           label = \"WARNING: Your Computer May Be Infected!\",\n           color = \"red\", size = 4, fontface = \"bold\",\n           hjust = 0.5) +\n  annotate(\"text\",\n           x = 1850, y = 0.2,\n           label = \"Call Now for Support:\\n1800-433-5055\",\n           color = \"blue\", size = 3,\n           hjust = 0.5) +\n  scale_shape_discrete(labels = c(\"Species\",\n                                  \"Second Species\",\n                                  \"Species C\")) +\n  scale_color_discrete(labels = c(\"Bisco\",\n                                  \"Island 2\",\n                                  \"Forgot the name\"))\n\n\n\n\n\nugly-vis\n\n\nPlot intentionally designed to be confusing and unpleasant, with unclear labels, unnecessary transformations, and distracting visual elements. The figure illustrates how poor design choices can hide patterns and mislead viewers rather than communicate information effectively.",
    "crumbs": [
      "Projects",
      "Data Visualization Gallery"
    ]
  }
]